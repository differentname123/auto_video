#!/usr/bin/env python
# -*- coding: utf-8 -*-
import random
import re
import traceback
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from types import SimpleNamespace
from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED
import requests
import time
import logging
import os
# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'
# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'
import json
import threading
from queue import Queue, Empty


from application.video_common_config import ALL_BILIBILI_EMOTE_PATH, TaskStatus
from utils.bilibili.bili_utils import update_bili_user_sign
from utils.bilibili.comment import BilibiliCommenter
from utils.bilibili.watch_video import watch_video
from utils.common_utils import get_config, read_json, init_config
from utils.mongo_base import gen_db_object
from utils.mongo_manager import MongoManager

bvid_file_path = '../../LLM/TikTokDownloader/back_up/bvid_file.json'
all_bvid_file_path = '../../LLM/TikTokDownloader/back_up/all_bvid_file.json'

interaction_data_file = '../../LLM/TikTokDownloader/back_up/interaction_data.json'

# --- 1. å…¨å±€å¸¸é‡ ---
URL_MODIFY_RELATION = "https://api.bilibili.com/x/relation/modify"

# --- 2. å…¨å±€é…ç½® ---
total_cookie = get_config("nana_bilibili_total_cookie")
csrf_token = get_config("nana_bilibili_csrf_token")

CONFIG = {
    "STRATEGIES": {
        "popular": False,  # çƒ­é—¨è§†é¢‘é€šå¸¸ä¸æ˜¯ç›®æ ‡ç”¨æˆ·ï¼Œå¯ä»¥å…³é—­
        "following": True,  # å·²ç»å…³æ³¨çš„UPä¸»ä¸éœ€è¦å†å¤„ç†
        "search": False,
        "ranking": False,  # <<< NEW: æ–°å¢åˆ†åŒºæ’è¡Œæ¦œç­–ç•¥å¼€å…³
    },
    "COOKIE": total_cookie,
    "CSRF_TOKEN": csrf_token,
    "TARGET_UIDS": [  # ç›‘æ§åŠ¨æ€æ—¶ä½¿ç”¨ï¼Œå½“å‰å·²å…³é—­
        "1223805908",
        "1639172564",
        "3546909677455941",
        "3546717871934392",
    ],
    # <<< NEW: START - æ–°å¢åˆ†åŒºæ’è¡Œæ¦œç›¸å…³é…ç½® >>>
    "RANKING_TIDS": {  # ç›®æ ‡åˆ†åŒºID (rid) å’Œåç§°çš„æ˜ å°„
        0: "å…¨ç«™",
        1: "åŠ¨ç”»",
        168: "å›½åˆ›",
        3: "éŸ³ä¹",
        129: "èˆè¹ˆ",
        4: "æ¸¸æˆ",
        36: "çŸ¥è¯†",
        188: "ç§‘æŠ€",
        234: "è¿åŠ¨",
        223: "æ±½è½¦",
        160: "ç”Ÿæ´»",
        211: "ç¾é£Ÿ",
        217: "åŠ¨ç‰©åœˆ",
        119: "é¬¼ç•œ",
        155: "æ—¶å°š",
        5: "å¨±ä¹",
        181: "å½±è§†",
    },
    # <<< NEW: END - æ–°å¢åˆ†åŒºæ’è¡Œæ¦œç›¸å…³é…ç½® >>>
    "TARGET_KEYWORDS": [
        "äº’å…³", "äº’ç²‰", "äº’èµ", "äº’åŠ©", "æ–°äººUPä¸»", "å›å…³", "å›ç²‰", "äº’æš–",
        "äº’è¯„", "äº’æ", "ä¸‰è¿", "æ±‚ä¸‰è¿", "äº’ä¸‰è¿", "äº’å¸", "æ–°äººæŠ¥é“", "æ–°äººup",
        "å°UPä¸»", "èŒæ–°UP", "åº•å±‚UPä¸»", "å°é€æ˜", "æ¶¨ç²‰", "æ±‚å…³æ³¨", "æ±‚æŠ±å›¢",
        "æŠ±å›¢å–æš–", "ä¸€èµ·åŠ æ²¹", "æŒ‘æˆ˜100ç²‰", "å†²å‡»åƒç²‰", "æœ‰ç²‰å¿…å›", "æœ‰èµå¿…å›",
        "åœ¨çº¿ç§’å›", "å·²å…³æ±‚å›"
    ],
    "FOLLOW_KEYWORDS": [
        "äº’å…³", "äº’ç²‰", "å›å…³", "äº’èµ", "äº’åŠ©", "å›ç²‰", "å¿…å›", "å¿…å›å…³",
        "æœ‰ç²‰å¿…å›", "æœ‰è®¿å¿…å›", "è¯šä¿¡äº’å…³", "è¯šä¿¡äº’ç²‰", "æ°¸ä¸å–å…³", "ä¸å–å…³",
        "èµè¯„å¿…å›", "äº’èµäº’è¯„", "äº’ä¸‰è¿", "äº’å¸", "å…³æˆ‘å¿…å›", "ç§ä¿¡ç§’å›",
        "ä½ å…³æˆ‘å°±å…³"
    ],
    "MAX_VIDEOS_PER_SOURCE": 20,  # æ¯æ¬¡æœç´¢/æ¯ä¸ªåˆ†åŒºæ’è¡Œå¯ä»¥å¤šæ‹‰å–ä¸€äº›
    "PROCESSED_VIDEOS_FILE": "comment_processed_bvideos.json",
    "GEN_PROCESSED_VIDEOS_FILE": "gen_comment_processed_bvideos.json",
    "COMMENTED_PROCESSED_VIDEOS_FILE": "commented_processed_bvideos.json",

    "PROCESSED_FIDS_FILE": "processed_fids.json",  # æ–°å¢ï¼šè®°å½•å·²å¤„ç†çš„ç”¨æˆ·ID
    "REQUEST_TIMEOUT": 10,
    "REQUEST_DELAY": 1,
}

# --- 3. æ—¥å¿—ä¸ä¼šè¯é…ç½® ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

# åˆ›å»ºä¸€ä¸ªå…¨å±€ä¼šè¯å¯¹è±¡ï¼Œç”¨äºä¿æŒç™»å½•çŠ¶æ€
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Referer': 'https://www.bilibili.com/',
    'Cookie': CONFIG['COOKIE']
})

import difflib
from typing import List, Optional


def most_similar_text(text_list: List[str], target_text: str) -> Optional[str]:
    """
    è¿”å› text_list ä¸­ä¸ target_text æœ€ä¸ºç›¸ä¼¼çš„å­—ç¬¦ä¸²ã€‚
    """
    if not text_list:
        return None

    best_match = '[åƒç“œ]'
    best_score = -1.0
    for text in text_list:
        score = difflib.SequenceMatcher(None, text, target_text).ratio()
        if score > best_score:
            best_score = score
            best_match = text

    return best_match


def replace_bracketed(text: str, text_list: List[str]) -> str:
    """
    æ‰¾åˆ° text ä¸­æ‰€æœ‰è¢« [ å’Œ ] åŒ…å›´çš„å­ä¸²ã€‚
    å¯¹äºå‰5ä¸ªå­ä¸²ï¼Œæå–å…¶ä¸­å†…å®¹ itemï¼Œç”¨ most_similar_text(text_list, item) çš„è¿”å›å€¼å»æ›¿æ¢æ•´ä¸ª [item]ã€‚
    å¯¹äºç¬¬6ä¸ªåŠä»¥åçš„ [ å’Œ ] å­ä¸²ï¼Œç›´æ¥åˆ é™¤ã€‚

    :param text: åŒ…å«è‹¥å¹² [â€¦] ç‰‡æ®µçš„åŸå§‹å­—ç¬¦ä¸²
    :param text_list: ç”¨äºåŒ¹é…çš„å€™é€‰å­—ç¬¦ä¸²åˆ—è¡¨
    :return: å¤„ç†åçš„æ–°å­—ç¬¦ä¸²
    """

    # åœ¨å¤–éƒ¨å‡½æ•°ä½œç”¨åŸŸå®šä¹‰ä¸€ä¸ªè®¡æ•°å™¨
    match_count = 0

    # å›è°ƒå‡½æ•°ï¼šä¸ºæ¯ä¸€ä¸ªåŒ¹é…é¡¹è®¡ç®—æ›¿æ¢ç»“æœ
    def _replacer(match: re.Match) -> str:
        nonlocal match_count
        match_count += 1

        # å¦‚æœæ˜¯å‰5ä¸ªåŒ¹é…é¡¹ï¼Œæ‰§è¡Œæ›¿æ¢é€»è¾‘
        if match_count <= 5:
            inner = match.group(1)
            best = most_similar_text(text_list, inner)
            # å¦‚æœæ²¡æ‰¾åˆ°ä»»ä½•åŒ¹é…ï¼Œä¿ç•™åŸæ‹¬å·å†…å®¹
            return best if best is not None else match.group(0)
        # å¦‚æœæ˜¯ç¬¬6ä¸ªåŠä»¥åçš„åŒ¹é…é¡¹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œå³åˆ é™¤è¯¥åŒ¹é…
        else:
            return ""

    # ä½¿ç”¨æ­£åˆ™æ›¿æ¢æ‰€æœ‰ [å†…å®¹]
    # re.sub ä¼šå¯¹æ¯ä¸€ä¸ªåŒ¹é…é¡¹è°ƒç”¨ä¸€æ¬¡ _replacer å‡½æ•°
    return re.sub(r'\[([^\]]+)\]', _replacer, text)


# --- 4. APIè¯·æ±‚æ ¸å¿ƒå‡½æ•° ---
def send_get_request(url, params=None):
    """é€šç”¨GETè¯·æ±‚å‡½æ•°"""
    try:
        # æ¯æ¬¡APIè¯·æ±‚å‰ï¼Œéšæœºæš‚åœ
        time.sleep(random.uniform(1.5, 3.5))
        response = session.get(url, params=params, timeout=CONFIG['REQUEST_TIMEOUT'])
        response.raise_for_status()
        data = response.json()
        if data.get('code', 0) != 0:
            logging.warning(f"APIè¿”å›é”™è¯¯: code={data.get('code')}, message={data.get('message')}, url={response.url}")
            return None
        return data.get('data')
    except requests.exceptions.RequestException as e:
        logging.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except json.JSONDecodeError:
        logging.error("æ— æ³•è§£ææœåŠ¡å™¨è¿”å›çš„JSONæ•°æ®ã€‚")
    return None


comment_danmu = [
    "æœ¬æ¥å·²ç»åˆ’èµ°äº†ï¼Œç»“æœçœ‹åˆ°ä¸€ä¸ªè¯„è®ºï¼Œè¿˜æ˜¯æ²¡å¿ä½å›æ¥ç‚¹èµã€‚",
    "é€€å‡ºå»åˆè¢«è¯„è®ºåŒºç‚¸å›æ¥äº†ï¼Œä½ ä»¬æ˜¯é­”é¬¼å—ï¼Ÿ",
    "è°¢è°¢è¯„è®ºåŒºï¼Œå·®ç‚¹å°±é”™è¿‡è¿™ä¸ªè§†é¢‘çš„ç²¾é«“äº†ã€‚",

    # â€œè®¤çŸ¥é¢ è¦†â€è§†è§’ (æš—ç¤ºè¯„è®ºåŒºæœ‰æƒŠå¤©å‘ç°æˆ–ä¸åŒè§£è¯»)
    "çœ‹å®Œè§†é¢‘ä¸€è„¸é—®å·ï¼Œçœ‹å®Œè¯„è®ºåŒºä¸€å¥å§æ§½ã€‚",
    "æˆ‘ä»¥ä¸ºæˆ‘æ‡‚äº†ï¼Œç›´åˆ°æˆ‘æ‰“å¼€äº†è¯„è®ºåŒºã€‚",
    "è¿™ä¸ªè§†é¢‘éœ€è¦æ­é…è¯„è®ºåŒºâ€œé£Ÿç”¨â€ï¼Œé£å‘³æ›´ä½³ã€‚",

    # â€œå¼ºçƒˆæ¨èâ€è§†è§’ (ç”¨ä¸ªäººæ„Ÿå—ä¸ºè¯„è®ºåŒºçš„ç²¾å½©ç¨‹åº¦èƒŒä¹¦)
    "è¯„è®ºåŒºç¬¬ä¸€æ¡ç›´æ¥ç»™æˆ‘å¹²æ²‰é»˜äº†ã€‚",
    "ä½ ä»¬å»çœ‹è¯„è®ºåŒºé‚£ä¸ªçƒ­è¯„ï¼Œæˆ‘ç¬‘åˆ°æ‰“å—ã€‚",
    "å¬è¯´è¯„è®ºåŒºæ¯”è§†é¢‘è¿˜ç²¾å½©ï¼Œç‰¹æ¥å›´è§‚ã€‚",

    "è§†é¢‘è¿˜æ²¡æŠŠæˆ‘æ€ä¹ˆæ ·ï¼Œè¯„è®ºåŒºå·®ç‚¹æŠŠæˆ‘ç¬‘èµ°ã€‚",
    "æˆ‘å®£å¸ƒï¼Œè¿™é‡Œæ˜¯ç¬¬ä¸€ç°åœºï¼Œè¯„è®ºåŒºæ˜¯ç¬¬äºŒç°åœºï¼",
    "è¿™ä¸ªè§†é¢‘çš„å¼¹å¹•ä¸€åŠï¼Œè¯„è®ºåŒºä¸€åŠï¼ŒUPä¸»åªè´Ÿè´£ä¸Šä¼ ã€‚",
]


def modify_relation(fid, action_type, csrf_token):
    """
    ä¿®æ”¹ç”¨æˆ·å…³ç³» (å…³æ³¨æˆ–å–æ¶ˆå…³æ³¨)ã€‚
    fid: ç›®æ ‡ç”¨æˆ·çš„UID
    action_type: 1 ä¸ºå…³æ³¨, 2 ä¸ºå–æ¶ˆå…³æ³¨
    csrf_token: ä»Cookieä¸­è·å–çš„bili_jctå€¼
    """
    action_text = "å…³æ³¨" if action_type == 1 else "å–æ¶ˆå…³æ³¨"
    payload = {
        "fid": fid,
        "act": action_type,
        "re_src": 11,  # å…³ç³»æ¥æºï¼Œé€šå¸¸ç”¨ 11
        "csrf": csrf_token
    }
    try:
        response = session.post(URL_MODIFY_RELATION, data=payload, timeout=CONFIG['REQUEST_TIMEOUT'])
        response.raise_for_status()
        result = response.json()
        if result.get('code') == 0:
            logging.info(f"  {'âœ…' if action_type == 1 else 'ğŸ—‘ï¸'} æˆåŠŸ{action_text} UID: {fid}")
            return True
        # å¸¸è§é”™è¯¯ç å¤„ç†
        elif result.get('code') == 22014:  # å¯¹æ–¹å°†ä½ æ‹‰é»‘
            logging.warning(f"  âš ï¸ {action_text} UID: {fid} å¤±è´¥: {result['message']} (å¯èƒ½å·²è¢«å¯¹æ–¹æ‹‰é»‘)")
            return True  # è¿”å›Trueï¼Œé¿å…é‡è¯•
        elif result.get('code') == 22007:  # å·²ç»å…³æ³¨äº†
            logging.info(f"  â„¹ï¸ {action_text} UID: {fid}: å·²ç»æ˜¯å…³æ³¨çŠ¶æ€ã€‚")
            return True  # è¿”å›Trueï¼Œé¿å…é‡è¯•
        else:
            logging.error(
                f"  âŒ {action_text} UID: {fid} å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')} (Code: {result.get('code')})")
            return False
    except requests.exceptions.RequestException as e:
        logging.error(f"  âŒ è¯·æ±‚{action_text} UID: {fid} å¤±è´¥: {e}")
        return False
    except ValueError:  # å¯¹åº” json.JSONDecodeError
        logging.error(f"  âŒ {action_text} UID: {fid} å“åº”å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSONã€‚")
        return False


# --- 5. è§†é¢‘è·å–ç­–ç•¥å®ç° ---
def fetch_from_popular(max_count=100):
    """
    å¾ªç¯è·å–Bç«™çƒ­é—¨æ¦œå•çš„è§†é¢‘ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šæ•°æ®ä¸ºæ­¢ã€‚
    """
    logging.info("å¼€å§‹æ‰§è¡Œ [ç­–ç•¥ä¸€ï¼šè·å–çƒ­é—¨è§†é¢‘]...")

    # å°† video_list åˆå§‹åŒ–åœ¨å¾ªç¯å¤–éƒ¨ï¼Œç”¨äºç´¯åŠ æ‰€æœ‰é¡µçš„æ•°æ®
    all_videos = []
    url = "https://api.bilibili.com/x/web-interface/popular"
    page_number = 1  # ä»ç¬¬ä¸€é¡µå¼€å§‹

    while True:
        logging.info(f"  > æ­£åœ¨å°è¯•è·å–çƒ­é—¨æ¦œå•ç¬¬ {page_number} é¡µ...")
        params = {'ps': CONFIG['MAX_VIDEOS_PER_SOURCE'], 'pn': page_number}

        data = send_get_request(url, params)

        # æ£€æŸ¥APIå“åº”æ˜¯å¦æˆåŠŸï¼Œå¹¶ä¸” 'list' é”®å­˜åœ¨ä¸”ä¸ä¸ºç©º
        if data and 'list' in data and data['list']:
            page_videos = data['list']
            for item in page_videos:
                if 'bvid' in item:
                    item['_source_strategy'] = 'popular'
                    all_videos.append(item)

            logging.info(f"  > æˆåŠŸä»ç¬¬ {page_number} é¡µè·å– {len(page_videos)} ä¸ªè§†é¢‘ã€‚")
            if len(all_videos) >= max_count:
                logging.info(f"  > å·²è¾¾åˆ°æœ€å¤§è·å–æ•°é‡ {max_count}ï¼Œåœæ­¢è·å–æ›´å¤šæ•°æ®ã€‚")
                break
            # å‡†å¤‡è·å–ä¸‹ä¸€é¡µ
            page_number += 1

            # å¢åŠ å»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«è¢«APIé™åˆ¶ã€‚å¯æ ¹æ®éœ€è¦è°ƒæ•´æ—¶é—´ã€‚
            time.sleep(1)
        else:
            # å¦‚æœ 'list' ä¸å­˜åœ¨ã€ä¸ºç©ºï¼Œæˆ–è€…APIè¯·æ±‚å¤±è´¥ï¼Œåˆ™è®¤ä¸ºæ²¡æœ‰æ›´å¤šæ•°æ®äº†
            logging.info("  > çƒ­é—¨æ¦œå•æ•°æ®å·²å…¨éƒ¨è·å–å®Œæ¯•ï¼Œæˆ–APIæœªè¿”å›æœ‰æ•ˆæ•°æ®ï¼Œåœæ­¢è·å–ã€‚")
            break  # é€€å‡ºå¾ªç¯

    if all_videos:
        logging.info(f"  > [ç­–ç•¥ä¸€ï¼šè·å–çƒ­é—¨è§†é¢‘] æ‰§è¡Œå®Œæ¯•ã€‚æ€»å…±è·å– {len(all_videos)} ä¸ªè§†é¢‘ã€‚")
    else:
        logging.warning("  > [ç­–ç•¥ä¸€ï¼šè·å–çƒ­é—¨è§†é¢‘] æ‰§è¡Œå®Œæ¯•ï¼Œä½†æœªèƒ½è·å–åˆ°ä»»ä½•è§†é¢‘ã€‚")

    return all_videos


def fetch_from_following():
    logging.info("å¼€å§‹æ‰§è¡Œ [ç­–ç•¥äºŒï¼šç›‘æ§å…³æ³¨çš„UPä¸»]...")
    if not CONFIG['TARGET_UIDS']:
        logging.warning("  > æœªé…ç½®ç›®æ ‡UIDï¼Œè·³è¿‡æ­¤ç­–ç•¥ã€‚")
        return []
    video_list = []
    url_template = "https://api.bilibili.com/x/polymer/web-dynamic/v1/feed/space"
    for uid in CONFIG['TARGET_UIDS']:
        logging.info(f"  > æ­£åœ¨è·å–UPä¸»(UID: {uid})çš„æœ€æ–°åŠ¨æ€...")
        params = {'host_mid': uid}
        data = send_get_request(url_template, params=params)
        if data and 'items' in data:
            found_count = 0
            for item in data['items']:
                if item.get('type') == 'DYNAMIC_TYPE_AV':
                    major = item.get('modules', {}).get('module_dynamic', {}).get('major')
                    if major and major.get('type') == 'MAJOR_TYPE_ARCHIVE':
                        video_data = major.get('archive')
                        if video_data and 'bvid' in video_data:
                            author_info = item.get('modules', {}).get('module_author', {})
                            video_data['owner'] = {
                                'mid': author_info.get('mid'),
                                'name': author_info.get('name'),
                                'face': author_info.get('face'),
                            }
                            # è¡¥å…¨midå­—æ®µï¼Œä¸æœç´¢ç»“æœå¯¹é½
                            if 'mid' not in video_data:
                                video_data['mid'] = author_info.get('mid')
                            video_data['_source_strategy'] = 'following'
                            video_list.append(video_data)
                            found_count += 1
                            if found_count >= CONFIG['MAX_VIDEOS_PER_SOURCE']: break
            logging.info(f"    - ä»UID {uid} å¤„è·å– {found_count} ä¸ªæ–°è§†é¢‘ã€‚")
    return video_list


def fetch_from_search():
    logging.info("å¼€å§‹æ‰§è¡Œ [ç­–ç•¥ä¸‰ï¼šå…³é”®è¯æœç´¢]...")
    if not CONFIG['TARGET_KEYWORDS']:
        logging.warning("  > æœªé…ç½®ç›®æ ‡å…³é”®è¯ï¼Œè·³è¿‡æ­¤ç­–ç•¥ã€‚")
        return []

    video_list = []
    url = "https://api.bilibili.com/x/web-interface/search/type"

    # å®šä¹‰æ¯é¡µè·å–çš„æ•°æ®é‡
    PAGE_SIZE = 20

    for keyword in CONFIG['TARGET_KEYWORDS']:
        logging.info(f"  > æ­£åœ¨æœç´¢å…³é”®è¯ '{keyword}'...")

        current_page = 1
        videos_fetched_for_keyword = 0  # è®°å½•å½“å‰å…³é”®è¯å·²è·å–çš„è§†é¢‘æ•°é‡

        while videos_fetched_for_keyword < CONFIG['MAX_VIDEOS_PER_SOURCE']:
            params = {
                'search_type': 'video',
                'keyword': keyword,
                'order': 'pubdate',  # æŒ‰æœ€æ–°å‘å¸ƒæ’åº
                'page': current_page,
                'ps': PAGE_SIZE  # å›ºå®šæ¯é¡µ20ä¸ª
            }

            logging.info(f"    - è¯·æ±‚ç¬¬ {current_page} é¡µï¼Œç›®æ ‡è·å– {PAGE_SIZE} ä¸ªè§†é¢‘...")
            data = send_get_request(url, params=params)

            if not data or 'result' not in data:
                logging.warning(
                    f"      - æœªèƒ½è·å–åˆ°å…³é”®è¯ '{keyword}' ç¬¬ {current_page} é¡µçš„æ•°æ®ï¼Œæˆ–æ•°æ®æ ¼å¼ä¸æ­£ç¡®ã€‚åœæ­¢æ­¤å…³é”®è¯çš„æœç´¢ã€‚")
                break  # æ— æ³•è·å–æ•°æ®ï¼Œåœæ­¢å½“å‰å…³é”®è¯çš„æœç´¢

            search_results = data.get('result', [])
            # å…¼å®¹è€ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬APIçš„è¿”å›æ ¼å¼
            if not isinstance(search_results, list):
                search_results = data.get('result', {}).get('video', [])

            if not search_results:
                logging.info(f"      - å…³é”®è¯ '{keyword}' ç¬¬ {current_page} é¡µæ²¡æœ‰æ›´å¤šè§†é¢‘äº†ã€‚")
                break  # å½“å‰é¡µæ²¡æœ‰æ•°æ®ï¼Œè¯´æ˜å·²ç»åˆ°å¤´äº†

            page_videos_added = 0  # è®°å½•å½“å‰é¡µå®é™…æ·»åŠ çš„è§†é¢‘æ•°é‡
            for item in search_results:
                if item.get('type') == 'video' and 'bvid' in item:
                    if 'title' in item:
                        item['title'] = item['title'].replace('<em class="keyword">', '').replace('</em>', '')
                    item['_source_strategy'] = 'search'
                    video_list.append(item)
                    videos_fetched_for_keyword += 1
                    page_videos_added += 1

                    # å¦‚æœå·²ç»è¾¾åˆ°æˆ–è¶…è¿‡äº†ç›®æ ‡æ•°é‡ï¼Œå°±åœæ­¢
                    if videos_fetched_for_keyword >= CONFIG['MAX_VIDEOS_PER_SOURCE']:
                        break  # è·³å‡º inner loop (for item in search_results)

            logging.info(
                f"      - ä»å…³é”®è¯ '{keyword}' ç¬¬ {current_page} é¡µè·å– {page_videos_added} ä¸ªè§†é¢‘ï¼Œå½“å‰å…³é”®è¯ç´¯è®¡ {videos_fetched_for_keyword} ä¸ªã€‚")

            # å¦‚æœå½“å‰é¡µè·å–çš„è§†é¢‘æ•°é‡å°‘äºPAGE_SIZEï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µäº†ï¼Œæˆ–è€…æ²¡æœ‰æ›´å¤šç¬¦åˆæ¡ä»¶çš„è§†é¢‘äº†
            if page_videos_added < PAGE_SIZE:
                logging.info(f"      - å…³é”®è¯ '{keyword}' å·²è·å–å®Œæ‰€æœ‰å¯ç”¨è§†é¢‘ï¼ˆä¸è¶³ {PAGE_SIZE} ä¸ªï¼‰ã€‚")
                break  # è·³å‡º outer loop (while videos_fetched_for_keyword < CONFIG.MAX_VIDEOS_PER_SOURCE)

            current_page += 1

            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«è¢«å°ç¦
            time.sleep(1)  # å»ºè®®å»¶è¿Ÿ1ç§’ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´

        logging.info(
            f"  > å…³é”®è¯ '{keyword}' æœç´¢å®Œæˆï¼Œæ€»å…±è·å– {videos_fetched_for_keyword} ä¸ªè§†é¢‘ (ç›®æ ‡ {CONFIG['MAX_VIDEOS_PER_SOURCE']})ã€‚")
        logging.info("-" * 50)  # åˆ†éš”çº¿
    CONFIG['MAX_VIDEOS_PER_SOURCE'] = 20  # é‡ç½®ä¸ºæ¯é¡µ20ä¸ªï¼Œé¿å…å½±å“åç»­æœç´¢ï¼Œå› ä¸ºä¸ä¼šæ›´æ–°è¿™ä¹ˆå¿«é€Ÿ
    return video_list


# <<< NEW: START - æ–°å¢åˆ†åŒºæ’è¡Œæ¦œè·å–å‡½æ•° >>>
def fetch_from_ranking():
    """
    ä»æŒ‡å®šåˆ†åŒºçš„æ’è¡Œæ¦œè·å–è§†é¢‘ã€‚
    """
    logging.info("å¼€å§‹æ‰§è¡Œ [ç­–ç•¥å››ï¼šè·å–åˆ†åŒºæ’è¡Œæ¦œè§†é¢‘]...")
    if not CONFIG['RANKING_TIDS']:
        logging.warning("  > æœªé…ç½®ç›®æ ‡åˆ†åŒºID (RANKING_TIDS)ï¼Œè·³è¿‡æ­¤ç­–ç•¥ã€‚")
        return []

    video_list = []
    url = "https://api.bilibili.com/x/web-interface/ranking/v2"

    for tid, name in CONFIG['RANKING_TIDS'].items():
        logging.info(f"  > æ­£åœ¨è·å–åˆ†åŒº '{name}' (TID: {tid}) çš„æ’è¡Œæ¦œ...")
        params = {
            'rid': tid,
            'type': 'all',  # è·å–å…¨éƒ¨åˆ†ç±»ï¼Œå¯æ ¹æ®éœ€æ±‚æ”¹ä¸º 'rookie' æˆ– 'origin'
        }

        data = send_get_request(url, params=params)

        if data and 'list' in data and data['list']:
            # APIè¿”å›æœ€å¤š100ä¸ªè§†é¢‘ï¼Œæˆ‘ä»¬æ ¹æ®é…ç½®å–å‰Nä¸ª
            ranking_videos = data['list']
            for item in ranking_videos:
                if 'bvid' in item:
                    item['_source_strategy'] = 'ranking'
                    video_list.append(item)
            logging.info(f"    - æˆåŠŸä»åˆ†åŒº '{name}' è·å– {len(ranking_videos)} ä¸ªè§†é¢‘ã€‚")
        else:
            logging.warning(f"    - æœªèƒ½ä»åˆ†åŒº '{name}' è·å–åˆ°è§†é¢‘æ•°æ®ï¼Œæˆ–æ•°æ®ä¸ºç©ºã€‚")

    if video_list:
        logging.info(f"  > [ç­–ç•¥å››ï¼šè·å–åˆ†åŒºæ’è¡Œæ¦œè§†é¢‘] æ‰§è¡Œå®Œæ¯•ã€‚æ€»å…±è·å– {len(video_list)} ä¸ªè§†é¢‘ã€‚")
    else:
        logging.warning("  > [ç­–ç•¥å››ï¼šè·å–åˆ†åŒºæ’è¡Œæ¦œè§†é¢‘] æ‰§è¡Œå®Œæ¯•ï¼Œä½†æœªèƒ½è·å–åˆ°ä»»ä½•è§†é¢‘ã€‚")

    return video_list


# <<< NEW: END - æ–°å¢åˆ†åŒºæ’è¡Œæ¦œè·å–å‡½æ•° >>>


# --- 6. å·²å¤„ç†è®°å½•ç®¡ç† (è§†é¢‘BVIDå’Œç”¨æˆ·FID) ---
def load_processed_set(filepath):
    if not os.path.exists(filepath):
        return set()
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return set(json.load(f))
    except (json.JSONDecodeError, IOError):
        return set()


def load_processed_dict(filepath):
    if not os.path.exists(filepath):
        return {}
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError):
        return {}


def save_processed_set(data_set, filepath):
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            # å°†é›†åˆè½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            json.dump(list(data_set), f, indent=4)
    except IOError as e:
        logging.error(f"ä¿å­˜æ–‡ä»¶ {filepath} å¤±è´¥: {e}")


def save_processed_dict(data_dict, filepath):
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            # å…³é”®æ”¹åŠ¨ï¼šæ·»åŠ  ensure_ascii=False
            json.dump(data_dict, f, indent=4, ensure_ascii=False)
        print(f"æ•°æ®å·²æˆåŠŸä¿å­˜åˆ° {filepath}")
    except IOError as e:
        logging.error(f"ä¿å­˜æ–‡ä»¶ {filepath} å¤±è´¥: {e}")


# --- 7. è§†é¢‘æ‹‰å–ä¸»é€»è¾‘ ---
def fetch_videos():
    logging.info("==================== å¼€å§‹è·å–å¾…å¤„ç†è§†é¢‘ ====================")
    processed_bvideos = load_processed_set(CONFIG['PROCESSED_VIDEOS_FILE'])
    # processed_bvideos = set()
    logging.info(f"å·²åŠ è½½ {len(processed_bvideos)} ä¸ªå·²å¤„ç†çš„è§†é¢‘è®°å½•ã€‚")

    all_found_videos = []
    if CONFIG['STRATEGIES']['popular']:
        all_found_videos.extend(fetch_from_popular())
    if CONFIG['STRATEGIES']['following']:
        all_found_videos.extend(fetch_from_following())
    if CONFIG['STRATEGIES']['search']:
        all_found_videos.extend(fetch_from_search())
    # <<< MODIFIED: START - é›†æˆæ–°çš„è·å–ç­–ç•¥ >>>
    if CONFIG['STRATEGIES']['ranking']:
        all_found_videos.extend(fetch_from_ranking())
    # <<< MODIFIED: END - é›†æˆæ–°çš„è·å–ç­–ç•¥ >>>

    unique_videos_map = {video['bvid']: video for video in reversed(all_found_videos) if 'bvid' in video}
    logging.info(f"æ‰€æœ‰ç­–ç•¥å…±æ‰¾åˆ° {len(all_found_videos)} ä¸ªè§†é¢‘ï¼Œå»é‡åå‰© {len(unique_videos_map)} ä¸ªã€‚")

    videos_to_process = [video for bvid, video in unique_videos_map.items() if bvid not in processed_bvideos]
    logging.info(f"è¿‡æ»¤æ‰å·²å¤„ç†çš„è§†é¢‘åï¼Œæœ€ç»ˆå¾—åˆ° {len(videos_to_process)} ä¸ªæ–°è§†é¢‘å¾…å¤„ç†ã€‚")

    newly_processed_bvid_set = {video['bvid'] for video in videos_to_process}
    updated_processed_set = processed_bvideos.union(newly_processed_bvid_set)
    save_processed_set(updated_processed_set, CONFIG['PROCESSED_VIDEOS_FILE'])
    logging.info(f"å·²å¤„ç†è§†é¢‘è®°å½•å·²æ›´æ–°ï¼Œæ€»æ•°: {len(updated_processed_set)}ã€‚")

    logging.info("==================== è·å–ä»»åŠ¡å®Œæˆ ====================")
    return videos_to_process


# --- 8. å¹¶å‘æ‰§è¡Œé€»è¾‘ ---
videos_queue = Queue()
comment_videos_queue = Queue()


def video_fetcher_worker():
    """è§†é¢‘æ‹‰å–çº¿ç¨‹ï¼šå®šæœŸæ‹‰å–æ–°è§†é¢‘å¹¶æ”¾å…¥é˜Ÿåˆ—ã€‚"""
    while True:
        new_videos = fetch_videos()
        if new_videos:
            # éšæœºæ‰“ä¹±é¡ºåºï¼Œé¿å…è¡Œä¸ºæ¨¡å¼è¿‡äºå›ºå®š
            random.shuffle(new_videos)
            for video in new_videos:
                videos_queue.put(video)
        else:
            logging.info("æœ¬æ¬¡æœªè·å–åˆ°æ–°è§†é¢‘ã€‚")
        logging.info(f'æœ¬æ¬¡è·å–åˆ° {len(new_videos)} ä¸ªæ–°è§†é¢‘ã€‚é˜Ÿåˆ—å½“å‰é•¿åº¦ï¼š{videos_queue.qsize()}')
        # æ¯æ¬¡æ‹‰å–å¤§å¾ªç¯ï¼Œéšæœºæš‚åœ20åˆ°30åˆ†é’Ÿ
        sleep_time = random.uniform(1200, 1800)
        logging.info(f"è§†é¢‘æ‹‰å–çº¿ç¨‹ä¼‘çœ  {int(sleep_time / 60)} åˆ†é’Ÿ...")
        time.sleep(sleep_time)


def get_comment_user(bvid):
    result_id_list = []
    try:
        comments = get_bilibili_comments(bvid)
        for i, reply in enumerate(comments):
            UID = reply['member']['mid']
            message = reply['content']['message']
            should_follow = any(keyword.lower() in message for keyword in CONFIG['FOLLOW_KEYWORDS'])
            if should_follow:
                result_id_list.append(UID)
    except Exception as e:
        logging.error(f"è·å–è¯„è®ºå¤±è´¥: {e}")
        return result_id_list
    return result_id_list


# (æ–°åŠŸèƒ½)
def gen_comment():
    """å…³æ³¨çº¿ç¨‹ï¼šä»é˜Ÿåˆ—è·å–è§†é¢‘ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å…³æ³¨ä½œè€…ã€‚"""
    detail_video_info_map = load_processed_dict(CONFIG['GEN_PROCESSED_VIDEOS_FILE'])
    processed_bvideos = load_processed_set(CONFIG['PROCESSED_VIDEOS_FILE'])
    # åªä¿ç•™processed_bvideosä¸­gen_commentä¸ä¸ºç©ºçš„è§†é¢‘
    detail_video_info_map = {bvid: info for bvid, info in detail_video_info_map.items() if info.get('gen_comment')}

    for bvid in processed_bvideos:
        if bvid not in detail_video_info_map:
            temp_dict = {}
            temp_dict['bvid'] = bvid
            videos_queue.put(temp_dict)

    logging.info(f"å·²åŠ è½½ {len(detail_video_info_map)} ä¸ªå·²ç”Ÿæˆçš„è®°å½•ã€‚")

    while True:
        try:
            video = videos_queue.get(timeout=30)  # ç­‰å¾…30ç§’ï¼Œå¦‚æœæ²¡æœ‰æ–°è§†é¢‘åˆ™ç»§ç»­å¾ªç¯
            logging.info(f"è·å–åˆ°æ–°è§†é¢‘ BVID: {video.get('bvid', 'æœªçŸ¥')}ï¼Œå¼€å§‹å¤„ç†...")
        except Empty:
            continue

        bvid = video.get('bvid')
        if bvid in detail_video_info_map.keys():
            logging.info(f"è§†é¢‘ BVID {bvid} å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡ã€‚")
            continue
        else:
            video_info = gen_proper_comment(bvid)
            if video_info:
                detail_video_info_map[bvid] = video_info
                save_processed_dict(detail_video_info_map, CONFIG['GEN_PROCESSED_VIDEOS_FILE'])
                logging.info(f"è§†é¢‘ BVID {bvid} å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜ç”Ÿæˆä¿¡æ¯ã€‚")
                comment_videos_queue.put(video_info)


def _deep_update(orig: dict, new: dict):
    """
    å°† new åˆå¹¶åˆ° origï¼š
    - å¦‚æœæŸä¸ª key åœ¨ orig å’Œ new ä¸­å¯¹åº”çš„ value éƒ½æ˜¯ dictï¼Œåˆ™é€’å½’åˆå¹¶ï¼›
    - å¦åˆ™ç›´æ¥ç”¨ new[key] è¦†ç›– orig[key]ï¼ˆæˆ–æ–°å¢ï¼‰ã€‚
    """
    for k, v in new.items():
        if k in orig and isinstance(orig[k], dict) and isinstance(v, dict):
            _deep_update(orig[k], v)
        else:
            orig[k] = v


def save_json(path: str, data: dict):
    """
    1. å…ˆåˆ›å»ºç›®å½•
    2. è¯»å·²æœ‰å†…å®¹ï¼ˆä¸å­˜åœ¨æˆ–è§£æå¤±è´¥å°±å½“ç©º dictï¼‰
    3. æ·±åº¦åˆå¹¶ data åˆ°å·²æœ‰å†…å®¹
    4. å†™å›æ–‡ä»¶ï¼ˆå¸¦ç¼©è¿›ã€ç¾åŒ–ï¼‰
    """
    # 1. ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)

    # 2. å°è¯•åŠ è½½å·²æœ‰æ–‡ä»¶
    try:
        with open(path, 'r', encoding='utf-8') as f:
            existing = json.load(f)
            if not isinstance(existing, dict):
                existing = {}
    except (FileNotFoundError, json.JSONDecodeError):
        existing = {}

    # 3. æ·±åº¦åˆå¹¶
    _deep_update(existing, data)

    # 4. å†™å›
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(existing, f, indent=4, ensure_ascii=False)


def find_video_by_bvid(bvid_to_find: str, data_dict: dict):
    """
    åœ¨ç»™å®šçš„å­—å…¸ä¸­æ ¹æ® bvid æŸ¥æ‰¾å¯¹åº”çš„è§†é¢‘ä¿¡æ¯ valueã€‚

    è¿™ä¸ªå‡½æ•°ä¼šéå†å­—å…¸ä¸­çš„æ¯ä¸€ä¸ª valueï¼Œå¹¶å®‰å…¨åœ°æ£€æŸ¥å…¶å†…éƒ¨æ˜¯å¦åŒ…å«
    'upload_info' -> 'upload_result' -> 'bvid' è¿™ä¸ªè·¯å¾„ï¼Œ
    ä¸”å…¶å€¼ä¸è¦æŸ¥æ‰¾çš„ bvid ç›¸åŒ¹é…ã€‚

    Args:
        bvid_to_find (str): éœ€è¦æŸ¥æ‰¾çš„ Bilibili è§†é¢‘ ID (bvid)ã€‚
        data_dict (dict): åŒ…å«å¤šä¸ªè§†é¢‘ä¿¡æ¯çš„æ•°æ®å­—å…¸ã€‚

    Returns:
        dict or None: å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›åŒ…å«è¯¥ bvid çš„æ•´ä¸ª valueï¼ˆå³è§†é¢‘ä¿¡æ¯å­—å…¸ï¼‰ï¼›
                      å¦‚æœéå†å®Œæ•´ä¸ªå­—å…¸éƒ½æ‰¾ä¸åˆ°ï¼Œåˆ™è¿”å› Noneã€‚
    """
    # éå†å­—å…¸ä¸­çš„æ¯ä¸€ä¸ªé”®å€¼å¯¹
    for key, video_info in data_dict.items():
        upload_info = video_info.get('upload_info', {})
        upload_result = upload_info.get('upload_result', {})
        found_bvid = upload_result.get('bvid')
        if found_bvid and found_bvid == bvid_to_find:
            return video_info
    return None


def find_video_by_title(title_to_find: str, data_dict: dict):
    for key, video_info in data_dict.items():
        upload_info = video_info.get('upload_info', {})
        upload_result = upload_info.get('upload_params', {})
        found_bvid = upload_result.get('title')
        if found_bvid and found_bvid == title_to_find:
            return video_info
    return None


def parse_and_group_danmaku(data: dict) -> list:
    """
    è§£æè¾“å…¥çš„å­—å…¸ï¼Œå°†å¼¹å¹•æŒ‰æ—¶é—´æˆ³è¿›è¡Œåˆ†ç»„ã€‚

    Args:
        data: åŒ…å«å¼¹å¹•ä¿¡æ¯çš„æºå­—å…¸ã€‚

    Returns:
        ä¸€ä¸ªæŒ‰æ—¶é—´æˆ³æ’åºçš„åˆ—è¡¨ã€‚æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œ
        åŒ…å« "å»ºè®®æ—¶é—´æˆ³" å’Œä¸€ä¸ªè¯¥æ—¶é—´æˆ³ä¸‹æ‰€æœ‰ "æ¨èå¼¹å¹•å†…å®¹" çš„åˆ—è¡¨ã€‚
    """
    # 1. ä½¿ç”¨ defaultdict(list) æ¥è‡ªåŠ¨å¤„ç†åˆ†ç»„
    grouped_danmaku = defaultdict(list)

    # 2. éå† "å¼€åœºå¼¹å¹•" å¹¶æ·»åŠ åˆ°åˆ†ç»„å­—å…¸ä¸­
    opening_danmaku = data.get("å¼€åœºå¼¹å¹•")
    if opening_danmaku:
        timestamp = opening_danmaku.get("å»ºè®®æ—¶é—´æˆ³")
        contents = opening_danmaku.get("æ¨èå¼¹å¹•å†…å®¹", [])
        if timestamp and contents:
            # ä½¿ç”¨ extend å°†åˆ—è¡¨ä¸­çš„æ‰€æœ‰å…ƒç´ éƒ½æ·»åŠ è¿›å»
            grouped_danmaku[timestamp].extend(contents)

    # 3. éå† "æ¨èå¼¹å¹•åˆ—è¡¨" å¹¶æ·»åŠ åˆ°åˆ†ç»„å­—å…¸ä¸­
    recommendation_list = data.get("æ¨èå¼¹å¹•åˆ—è¡¨", [])
    recommendation_list_back = data.get("ç²¾é€‰å¼¹å¹•å†åˆ›ä½œåˆ—è¡¨", [])
    recommendation_list.extend(recommendation_list_back)
    for item in recommendation_list:
        timestamp = item.get("å»ºè®®æ—¶é—´æˆ³")
        contents = item.get("æ¨èå¼¹å¹•å†…å®¹", [])
        if timestamp and contents:
            grouped_danmaku[timestamp].extend(contents)

    # 4. å°†åˆ†ç»„åçš„å­—å…¸è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼çš„åˆ—è¡¨
    final_list = []
    for timestamp, contents in grouped_danmaku.items():
        final_list.append({
            "å»ºè®®æ—¶é—´æˆ³": timestamp,
            "æ¨èå¼¹å¹•å†…å®¹": contents
        })

    # 5. æŒ‰æ—¶é—´æˆ³å¯¹æœ€ç»ˆåˆ—è¡¨è¿›è¡Œæ’åº
    final_list.sort(key=lambda x: x["å»ºè®®æ—¶é—´æˆ³"])

    return final_list


danmu_praises_general_quality = [
    # --- 1. æåº¦é€šç”¨å‹ (å‡ ä¹é€‚ç”¨äºæ‰€æœ‰éåŠ£è´¨è§†é¢‘) ---
    "UPä¸»ç”¨å¿ƒäº†",
    "è¿™ä¸ªè§†é¢‘åšå¾—çœŸå¥½",
    "è´¨é‡ä¸é”™ï¼Œæ”¯æŒä¸€ä¸‹",
    "è§‚æ„Ÿå¾ˆèˆ’æœ",
    "å¥½è¯„ï¼",
    "åˆ¶ä½œä¸æ˜“ï¼Œç»™ä½ ç‚¹èµäº†",
    "æ„Ÿè§‰å¾ˆæµç•…",
    "çœ‹å¾—å‡ºæ¥æ˜¯è®¤çœŸåšçš„",
    "è¿™ä¸ªè´¨é‡å¯ä»¥çš„",
    "ä¸é”™ä¸é”™",

    # --- 2. å¤¸èµå‰ªè¾‘ä¸èŠ‚å¥ ---
    "è¿™å‰ªè¾‘ï¼Œæœ‰ç‚¹ä¸œè¥¿",
    "èŠ‚å¥å¾ˆæ£’ï¼Œä¸çŸ¥ä¸è§‰å°±çœ‹å®Œäº†",
    "è½¬åœºå¥½è‡ªç„¶",
    "BGMå’Œç”»é¢é…åˆå¾—çœŸå¥½",
    "è¿™ä¸ªå‰ªè¾‘èŠ‚å¥çˆ±äº†",
    "ä¿¡æ¯å¯†åº¦åˆšåˆšå¥½ï¼Œä¸æ‹–æ²“",
    "ç¥ä»™å‰ªè¾‘ï¼",

    # --- 3. å¤¸èµç”»é¢ä¸è§†å¬ä½“éªŒ (éç‰¹æŒ‡é«˜æ¸…) ---
    "ç”»é¢å¾ˆå¹²å‡€",
    "çœ‹ç€å¾ˆæ¸…çˆ½",
    "é•œå¤´å¾ˆç¨³ï¼Œå¥½è¯„",
    "è¿™ä¸ªæ„å›¾å­¦åˆ°äº†",
    "å­—å¹•å¥½è¯„ï¼Œçœ‹å¾—èˆ’æœå¤šäº†",
    "æ”¶éŸ³å¾ˆæ¸…æ™°ï¼Œæ²¡æœ‰æ‚éŸ³",
    "å­—ä½“å’Œæ’ç‰ˆå¥½è¯„",
    "bgmå¥½å¬ï¼Œæ±‚bgmï¼",  # ä¾§é¢å¤¸èµå“å‘³

    # --- 4. å¤¸èµæ•´ä½“è´¨æ„Ÿä¸æ°›å›´ ---
    "è´¨æ„Ÿæ‹‰æ»¡äº†",
    "æœ‰ç”µå½±æ„Ÿäº†",  # æ³›æŒ‡ï¼Œä¸ä¸€å®šæ˜¯çœŸçš„ç”µå½±æœº
    "è¿™è§†é¢‘æœ‰ç§é«˜çº§æ„Ÿ",
    "èµå¿ƒæ‚¦ç›®",
    "å®Œæˆåº¦å¥½é«˜å•Š",
    "æ˜¯ä¸ªå®è—UPä¸»",

    # --- 5. äº’åŠ¨ä¸é¼“åŠ±å‹ ---
    "è¿™è´¨é‡ï¼Œå€¼å¾—ä¸€ä¸ªä¸‰è¿ï¼",
    "æœæ–­ä¸‰è¿äº†",
    "å·²å…³æ³¨ï¼ŒæœŸå¾…æ›´å¤šå¥½ä½œå“",
    "å¥½æ´»ï¼Œå½“èµï¼",  # åäºŒæ¬¡å…ƒ/Bç«™é£æ ¼
    "ä½ æ›´æ–°ï¼Œæˆ‘ä¸‰è¿ï¼Œå°±è¿™ä¹ˆå®šäº†",
    "è¿™ä¸å¾—ç‹ ç‹ ç‚¹ä¸ªèµ",
    "ç ä½ï¼Œå›å¤´å†çœ‹ä¸€é",  # è¡¨è¾¾å¯¹è§†é¢‘è´¨é‡çš„è®¤å¯
]


def filter_danmu(danmu_list, duration):
    """
    è¿‡æ»¤å’Œè°ƒæ•´å¼¹å¹•åˆ—è¡¨ã€‚
    1. ç¡®ä¿æ‰€æœ‰å¼¹å¹•çš„æ—¶é—´æˆ³åœ¨è§†é¢‘æ—¶é•¿èŒƒå›´å†…ï¼Œæ— æ•ˆæ—¶é—´æˆ³ä¼šéšæœºåˆ†é…ã€‚
    2. å¦‚æœæœ€ç»ˆå¼¹å¹•æ•°é‡ä¸è¶³25æ¡ï¼Œåˆ™ä»é€šç”¨å¼¹å¹•æ± ä¸­éšæœºæŠ½å–è¡¥è¶³ã€‚

    Args:
        danmu_list: å¼¹å¹•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯åŒ…å« 'å»ºè®®æ—¶é—´æˆ³' å’Œ 'æ¨èå¼¹å¹•å†…å®¹' çš„å­—å…¸ã€‚
        duration: è§†é¢‘æ€»æ—¶é•¿ï¼Œæ ¼å¼ä¸º "HH:MM:SS" æˆ– "MM:SS" æˆ–ç§’æ•°ã€‚

    Returns:
        è°ƒæ•´åçš„å¼¹å¹•åˆ—è¡¨ï¼Œè‡³å°‘æœ‰25æ¡å¼¹å¹•ï¼ˆé™¤éè§†é¢‘æ—¶é•¿æ— æ•ˆï¼‰ã€‚
    """
    common_danmu_list = [
        "å±å¹•é‚£å¤´çš„é™Œç”Ÿäººï¼Œä¸ç®¡ä½ åœ¨å“ªé‡Œï¼Œç¥ä½ å¤©å¤©å¼€å¿ƒã€‚",
        "ç¥åˆ·åˆ°è¿™æ¡è§†é¢‘çš„ä½ ï¼Œçƒ¦æ¼å…¨æ¶ˆï¼Œæœªæ¥å¯æœŸã€‚",
        "æ„¿åˆ·åˆ°è¿™é‡Œçš„ä½ ï¼Œå‡›å†¬æ•£å°½ï¼Œæ˜Ÿæ²³é•¿æ˜ã€‚",
        "å¸Œæœ›è¿™æ¡å¼¹å¹•èƒ½å¸æ”¶ä½ ä»Šå¤©æ‰€æœ‰çš„ä¸å¼€å¿ƒã€‚",
        "è¿™æ¡å¼¹å¹•ä¸ä¸ºä»€ä¹ˆï¼Œå°±æ˜¯æƒ³ç¥ä½ ä¸‡äº‹èƒœæ„ã€‚",

        "å¤–é¢åœ¨ä¸‹é›¨ï¼Œå±‹é‡Œçœ‹è§†é¢‘ï¼Œæ„Ÿè§‰å¾ˆå®‰å¿ƒã€‚",
        "è¿™é‡Œæ˜¯å¼¹å¹•è®¸æ„¿æ± ï¼Œè®¸ä¸ªæ„¿å§ï¼Œä¸‡ä¸€å®ç°äº†å‘¢ï¼Ÿ",
        "æ„Ÿè§‰ç´¯äº†ï¼Œå¤§å®¶èƒ½åœ¨è¿™é‡Œç•™ä¸‹ä¸€å¥åŠ æ²¹å—ï¼Ÿç»™æˆ‘ä¹Ÿç»™ä½ è‡ªå·±ã€‚",
        "æˆ‘çš„ç”µé‡æ¯”è¿›åº¦æ¡è¿˜å¤šï¼Œä¼˜åŠ¿åœ¨æˆ‘ï¼",
        "å‰æ–¹é«˜èƒ½ï¼",
        "ç™½å«–å¤±è´¥ï¼ŒæŠ•å¸äº†æŠ•å¸äº†",
        "ç»™å±å¹•å¯¹é¢é‚£ä¸ªæˆ–è®¸æœ‰äº›ç–²æƒ«çš„ä½ ï¼Œä¸€ä¸ªçœ‹ä¸è§çš„æ‹¥æŠ±ã€‚",
        "ä»Šå¤©ä¹Ÿè¦å¥½å¥½åƒé¥­ï¼Œå¥½å¥½ç”Ÿæ´»å‘€ï¼",
        "å¾ˆé«˜å…´åœ¨æ­¤åˆ»ï¼Œä¸å±å¹•å‰çš„å„ä½â€œç½‘å‹â€å…±åº¦è¿™ä¸€åˆ†ä¸€ç§’ã€‚",
        "æŠŠä¸å¼€å¿ƒçš„äº‹ï¼Œéƒ½ç•™åœ¨å½“ä¸‹å§ï¼",
        "è®©è¿™æ¡å¼¹å¹•å¸¦èµ°ä½ ä»Šå¤©çš„ç–²æƒ«ã€‚",
    ]

    danmaku_zouxin_sanlian_gongmian = [
        "å°±å†²ç»“å°¾è¿™å¥ï¼Œæ”¾å¿ƒæŠŠä¸‰è¿äº¤äº†",
        "ä¸‰è¿é€ä¸Šï¼Œè¿™ç»“å°¾å¤ªå€¼å¾—",
        "æœ€åä¸€æ®µå€¼å¾—ä¸‰è¿æ”¶è—",
        "è¿™å¥ç¥ç¦è®©æˆ‘æ¯«ä¸çŠ¹è±«ä¸‰è¿",
        "æŠŠè¿™æ®µå½“æˆä»Šæ—¥å°ç¡®å¹¸ï¼Œä¸‰è¿å·²äº¤ä»˜",
        "è¿™ç»“å°¾å€¼å¾—å¤šæŒ‰å‡ ä¸‹ï¼ˆå·²æŒ‰ï¼‰",
        "å·²ä¸‰è¿ï¼Œæ„¿è¿™ä»½ç¥ç¦å¸¸åœ¨",
        "æ‚„æ‚„ä¸‰è¿ï¼Œæœ€åä¸€å¥åå¤å›æ”¾ä¸­",
        "è¢«æœ€åè¿™å¥æ²»æ„ˆäº†ï¼Œä¸‰è¿å¿…é¡»çš„",
        "æœ€åè¿™å¥å€¼å¾—ä¸‰è¿ä¹Ÿå€¼å¾—æ”¶è—",
        "ä¸‰è¿å·²ç»™ï¼Œæ„Ÿæ©è¿™ä»½æ¸©æŸ”",
        "æ‰‹æ»‘ä¸‰è¿äº†ï¼ˆæ˜¯çœŸçš„èµ°å¿ƒï¼‰",
        "è¿™ç¥ç¦åƒæš–é˜³ï¼Œç…§è¿›çƒ¦å¿ƒå¤„",
        "ä¸€å¥èµ°å¿ƒè¯ï¼Œæ•´å¤©éƒ½èˆ’æœäº†"
    ]
    try:
        total_seconds = time_to_ms(duration) / 1000
        total_seconds = int(total_seconds)
    except Exception as e:
        total_seconds = None
    if total_seconds is None or total_seconds <= 0:
        return danmu_list

    # === ç¬¬ä¸€æ­¥ï¼šå¤„ç†å¹¶è§„èŒƒåŒ–ä¼ å…¥çš„å¼¹å¹•åˆ—è¡¨ ===
    processed_danmu = []
    for item in danmu_list:
        try:
            # ä¸ºäº†ä¸ä¿®æ”¹åŸå§‹åˆ—è¡¨ï¼Œåˆ›å»ºä¸€ä¸ªå‰¯æœ¬è¿›è¡Œæ“ä½œ
            new_item = item.copy()
            ts = new_item.get('å»ºè®®æ—¶é—´æˆ³')
            seconds = time_to_ms(ts) / 1000
            seconds = int(seconds) if seconds is not None else None

            # å¦‚æœæ—¶é—´æˆ³æ— æ³•è§£ææˆ–è¶…å‡ºèŒƒå›´ï¼Œåˆ™éšæœºåˆ†é…
            if seconds is None or seconds < 0 or seconds > total_seconds:
                seconds = random.randint(2, total_seconds - 10)

            new_item['å»ºè®®æ—¶é—´æˆ³'] = seconds
            processed_danmu.append(new_item)
        except Exception as e:
            logging.error(f"å¤„ç†å¼¹å¹•æ—¶å‡ºé”™: {e}")
            continue

    processed_danmu_count = 0
    for item in processed_danmu:
        if isinstance(item.get('æ¨èå¼¹å¹•å†…å®¹'), list):
            processed_danmu_count += len(item['æ¨èå¼¹å¹•å†…å®¹'])

    target_num = 25
    # === ç¬¬äºŒæ­¥ï¼ˆæ–°å¢é€»è¾‘ï¼‰ï¼šæ£€æŸ¥å¼¹å¹•æ•°é‡å¹¶è¡¥è¶³åˆ°25æ¡ ===
    num_to_add = target_num - processed_danmu_count
    num_to_add = min(num_to_add, len(common_danmu_list))  # é¿å…è¶…å‡ºé€šç”¨æ± èŒƒå›´
    if num_to_add > 0:
        print(f"å¼¹å¹•æ•°é‡ä¸º {processed_danmu_count}ï¼Œä¸è¶³{target_num}æ¡ï¼Œéœ€è¦è¡¥å…… {num_to_add} æ¡ã€‚")
        # ä»é€šç”¨å¼¹å¹•æ± ä¸­éšæœºé€‰æ‹© num_to_add æ¡
        random_choices = random.sample(common_danmu_list, k=num_to_add)

        for content in random_choices:
            # 2. åœ¨è§†é¢‘æ—¶é•¿èŒƒå›´å†…éšæœºåˆ†é…ä¸€ä¸ªæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
            timestamp = random.randint(2, total_seconds - 10)

            # 3. åˆ›å»ºæ–°çš„å¼¹å¹•å­—å…¸å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            new_danmu = {
                'å»ºè®®æ—¶é—´æˆ³': timestamp,
                'æ¨èå¼¹å¹•å†…å®¹': [content]
            }
            processed_danmu.append(new_danmu)

    # å¢åŠ å›ºå®šçš„ä¸‰è¿å¼¹å¹•
    random_choices = random.sample(danmaku_zouxin_sanlian_gongmian, k=2)
    time_diff = 6
    for content in random_choices:
        new_danmu = {
            'å»ºè®®æ—¶é—´æˆ³': total_seconds - time_diff,
            'æ¨èå¼¹å¹•å†…å®¹': [content]
        }
        time_diff += 4
        processed_danmu.append(new_danmu)
    return processed_danmu


def extract_guides(data):
    """
    ä»ç»™å®šçš„æ•°æ®å­—å…¸ä¸­æå–â€œäº’åŠ¨å¼•å¯¼â€å’Œâ€œè¡¥å……ä¿¡æ¯â€åˆ—è¡¨ã€‚

    å‚æ•°:
        data: dict
            æ•°æ®ç»“æ„ä¸­æ¯ä¸ªé¡¶å±‚ key å¯¹åº”ä¸€ä¸ªæ–¹æ¡ˆï¼Œæ–¹æ¡ˆå†…å¯èƒ½åŒ…å«â€œç®€ä»‹â€å­—å…¸ï¼Œ
            å…¶ä¸‹åŒ…å«â€œäº’åŠ¨å¼•å¯¼â€å’Œâ€œè¡¥å……ä¿¡æ¯â€å­—æ®µã€‚

    è¿”å›:
        Tuple[List[str], List[str]]
            ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯æ‰€æœ‰æ–¹æ¡ˆçš„â€œäº’åŠ¨å¼•å¯¼â€åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯æ‰€æœ‰æ–¹æ¡ˆçš„â€œè¡¥å……ä¿¡æ¯â€åˆ—è¡¨ã€‚
            å¦‚æœæ²¡æœ‰å¯¹åº”å­—æ®µï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    interaction_prompts = []

    supplementary_notes = []

    for scheme_name, scheme_content in data.items():
        # è·å–â€œç®€ä»‹â€éƒ¨åˆ†
        brief = scheme_content.get("ç®€ä»‹", {})
        # æå–äº’åŠ¨å¼•å¯¼
        prompt = brief.get("äº’åŠ¨å¼•å¯¼")
        if isinstance(prompt, str) and prompt.strip():
            interaction_prompts.append(prompt.strip())
        # æå–è¡¥å……ä¿¡æ¯
        note = brief.get("è¡¥å……ä¿¡æ¯")
        if isinstance(note, str) and note.strip():
            supplementary_notes.append(note.strip())

    return interaction_prompts, supplementary_notes


def format_bilibili_emote(comment_list, all_emote_list):
    """
    è¿›è¡Œbç«™çš„emoteè½¬æ¢ï¼Œé¿å…æ²¡æœ‰æ­£å¸¸è¾“å‡ºè¡¨æƒ…
    """
    for comment in comment_list:
        # å°†ç¬¬ä¸€ä¸ªå…ƒç´ è°ƒç”¨ replace_bracketed
        comment[0] = replace_bracketed(comment[0], all_emote_list)


def generate_danmaku_plan(total_duration: int, text_list: list, target_num: int = 4) -> list:
    """
    åœ¨ total_duration èŒƒå›´å†…éšæœºç”Ÿæˆ target_num ä¸ªå¼¹å¹•è®¡åˆ’ï¼ˆæ—¶é—´æˆ³ä¸ºæ•´æ•°ç§’ï¼‰

    å‚æ•°:
        total_duration (int): è§†é¢‘æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
        text_list (list[str]): å¯ä¾›é€‰æ‹©çš„å¼¹å¹•å†…å®¹
        target_num (int): éœ€è¦ç”Ÿæˆçš„å¼¹å¹•æ•°é‡ï¼Œé»˜è®¤ä¸º4

    è¿”å›:
        list[dict]: æ¯ä¸ªå…ƒç´ åŒ…å« 'å»ºè®®æ—¶é—´æˆ³' å’Œ 'æ¨èå¼¹å¹•å†…å®¹'
    """
    if not text_list:
        raise ValueError("text_listä¸èƒ½ä¸ºç©º")
    if target_num > len(text_list):
        target_num = len(text_list)  # é¿å…è¶…å‡ºå¯é€‰èŒƒå›´

    # éšæœºé€‰å– target_num ä¸ªå¼¹å¹•å†…å®¹
    chosen_texts = random.sample(text_list, target_num)

    # éšæœºç”Ÿæˆä¸é‡å¤çš„æ—¶é—´æˆ³ï¼ˆæ•´æ•°ç§’ï¼‰
    chosen_timestamps = sorted(random.sample(range(total_duration + 1), target_num))

    # æ‹¼æ¥ç»“æœ
    result = []
    for ts, text in zip(chosen_timestamps, chosen_texts):
        result.append({
            "å»ºè®®æ—¶é—´æˆ³": ts,
            "æ¨èå¼¹å¹•å†…å®¹": [text]
        })

    return result


def gen_hudong_info(bvid, interaction_data, metadata_cache_with_uploads, all_emote_list):
    """
    ä¸º bvid ç”Ÿæˆç›¸åº”çš„æ¨èå¼¹å¹•ä¸è¯„è®ºï¼Œå¢å¼ºäº†å¯¹ None å’Œç¼ºå¤±å­—æ®µçš„å®¹é”™èƒ½åŠ›ã€‚
    """
    try:
        target_value = find_video_by_bvid(bvid, metadata_cache_with_uploads) or {}
    except Exception as e:
        # å‘ç”Ÿå¼‚å¸¸æ—¶è®°å½•æˆ–æ‰“å° eï¼ˆå¯é€‰ï¼‰ï¼Œå¹¶ä½¿ç”¨ç©º dict ç»§ç»­
        # logger.warning(f"find_video_by_bvid error for {bvid}: {e}")
        target_value = {}
    if target_value.get('hudong', {}) == {}:
        return {}
    hudong_info = {}
    # 1. å¦‚æœå·²æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    existing = interaction_data.get(bvid, {})
    if existing and 'hudong' in existing:
        hud = existing['hudong']
        hudong_info = existing['hudong']
        if target_value.get('hudong', {}).get('comment_list', []) == []:
            if hud:
                return hud

    # 2. å®‰å…¨è°ƒç”¨ find_video_by_bvid

    duration = target_value.get('metadata', [{}])[0].get('duration', '00:02')
    try:
        total_seconds = time_to_ms(duration) / 1000
        comment_list = target_value.get('hudong', {}).get('comment_list', []) or []
        # å¦‚æœè¯„è®ºåˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨ gen_proper_comment ç”Ÿæˆ
        if not comment_list:
            gen_info = gen_proper_comment(bvid) or {}
            if duration == '00:02':
                duration = gen_info.get('æ€»æ—¶é•¿', '00:02')

            raw_comments = gen_info.get('gen_comment', [])
            # å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬ä¸º (comment, weight, extra) ç»“æ„
            comment_list = [[c, 1, "None"] for c in raw_comments]
    except Exception as e:
        comment_list = []
        print(f"å¤„ç†è¯„è®ºåˆ—è¡¨æ—¶å‡ºé”™: {e}")

    try:
        danmu_info = target_value.get('hudong', {}).get('danmu_info', {})
        if danmu_info:
            danmu_list = parse_and_group_danmaku(danmu_info)
        else:
            # fallback: ä¸€æ¡é€šç”¨å¼¹å¹•
            danmu_list = [{'å»ºè®®æ—¶é—´æˆ³': '00:01', 'æ¨èå¼¹å¹•å†…å®¹': danmu_praises_general_quality}]
    except Exception as e:
        # logger.error(f"å¤„ç†å¼¹å¹•åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        danmu_list = [{'å»ºè®®æ—¶é—´æˆ³': '00:01', 'æ¨èå¼¹å¹•å†…å®¹': danmu_praises_general_quality}]
    danmu_list = filter_danmu(danmu_list, duration)
    total_seconds = int(total_seconds)

    title_schemes = target_value.get('title_schemes', {})
    interaction_prompts, supplementary_notes = extract_guides(title_schemes)  # æå–äº’åŠ¨å¼•å¯¼å’Œè¡¥å……ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if len(interaction_prompts) == 0:
        interaction_prompts = ["åˆ·åˆ°è¿™ä¸ªè§†é¢‘çš„ä½ ï¼Œå¸Œæœ›ä»Šå¤©èƒ½æœ‰ä¸ªå¥½å¿ƒæƒ…å‘€~",
                               "å®ï¼ä½ æ”¶åˆ°ä¸€ä»½æ¥è‡ªUPä¸»çš„å¥½è¿ï¼Œè¯·æ³¨æ„æŸ¥æ”¶å“¦ï¼",
                               "ä¸ç®¡æ­¤åˆ»ä½ åœ¨åšä»€ä¹ˆï¼Œéƒ½è¦è®°å¾—å¥½å¥½ç…§é¡¾è‡ªå·±ã€‚",
                               "å˜¿ï¼Œæœ‹å‹ï¼Œä¸ºä½ æ­£åœ¨ä»˜å‡ºçš„ä¸€åˆ‡ç‚¹èµï¼Œä½ è¶…æ£’çš„ï¼",
                               "å¾ˆé«˜å…´é‡è§ä½ ï¼Œæ„¿æ‰€æœ‰ç¾å¥½éƒ½å‘ä½ å¥”èµ´è€Œæ¥ã€‚"]
    if len(supplementary_notes) == 0:
        supplementary_notes = ["æ„Ÿè°¢ä½ æ„¿æ„èŠ±æ—¶é—´çœ‹åˆ°æœ€åï¼Œæ„¿è¿™ä»½å¥½è¿èƒ½ä¸€ç›´é™ªç€ä½ ã€‚",
                               "å¦‚æœè§‰å¾—è§†é¢‘è¿˜ä¸é”™ï¼Œä¸å¦¨ç‚¹ä¸ªèµï¼ŒæŠŠè¿™ä»½å¿«ä¹å’Œç¥ç¦ä¸€èµ·å¸¦èµ°å§ï¼",
                               "è§†é¢‘è™½å·²ç»“æŸï¼Œä½†æˆ‘çš„ç¥ç¦ä¸ä¼šã€‚ç¥ä½ ï¼Œä¸æ­¢ä»Šå¤©ï¼Œå¤©å¤©å¼€å¿ƒï¼",
                               "æ„Ÿè°¢æˆ‘ä»¬çš„è¿™æ¬¡ç›¸é‡ï¼Œæˆ‘ä»¬ä¸‹æœŸå†è§ï¼Œåœ¨é‚£ä¹‹å‰ï¼Œè¦ä¸€åˆ‡é¡ºåˆ©å“¦ï¼",
                               "é‚£ä¹ˆï¼Œå°±åˆ°è¿™é‡Œå•¦ã€‚æ™šå®‰ï¼Œç¥ä½ å¥½æ¢¦ï¼Œå¿˜æ‰æ‰€æœ‰çƒ¦æ¼ã€‚"]
    interaction_danmu_list = [{'å»ºè®®æ—¶é—´æˆ³': 1, 'æ¨èå¼¹å¹•å†…å®¹': interaction_prompts}]
    supplementary_notes_list = [{'å»ºè®®æ—¶é—´æˆ³': total_seconds - 8, 'æ¨èå¼¹å¹•å†…å®¹': supplementary_notes}]
    owner_danmu_list = []  # ç”¨äºå­˜å‚¨UPä¸»çš„å¼¹å¹•
    owner_danmu_list.extend(interaction_danmu_list)  # å°†äº’åŠ¨å¼•å¯¼å¼¹å¹•æ·»åŠ åˆ°UPä¸»å¼¹å¹•åˆ—è¡¨ä¸­
    owner_danmu_list.extend(supplementary_notes_list)  # å°†è¡¥å……ä¿¡æ¯å¼¹å¹•æ·»åŠ åˆ°UPä¸»å¼¹å¹•åˆ—è¡¨ä¸­
    format_bilibili_emote(comment_list, all_emote_list)
    # 5. ç»„è£…ç»“æœï¼Œå†™å›ç¼“å­˜ï¼Œå¹¶è¿”å›
    hudong_info["duration"] = total_seconds
    hudong_info['comment_list'] = comment_list
    hudong_info['danmu_list'] = danmu_list
    hudong_info['owner_danmu'] = owner_danmu_list
    # å†™å› interaction_data æ—¶åŒ…è£¹åœ¨ 'hudong' å­—æ®µé‡Œï¼Œä»¥ä¿æŒä¸å…¥å£é€»è¾‘ä¸€è‡´
    interaction_data[bvid] = {'hudong': hudong_info}

    return hudong_info


def path_exists(path) -> bool:
    """
    åˆ¤æ–­è¾“å…¥çš„è·¯å¾„å­—ç¬¦ä¸²æ˜¯å¦å­˜åœ¨ã€‚

    å‚æ•°:
        path: è·¯å¾„å­—ç¬¦ä¸²æˆ– Noneã€‚

    è¿”å›:
        å¦‚æœ path æ˜¯éç©ºå­—ç¬¦ä¸²ä¸”å¯¹åº”è·¯å¾„å­˜åœ¨ï¼Œåˆ™è¿”å› Trueï¼›
        å¦åˆ™è¿”å› Falseã€‚
    """
    # æ’é™¤ None å’Œéå­—ç¬¦ä¸²
    if not isinstance(path, str):
        return False

    # å»é™¤é¦–å°¾ç©ºç™½åä¸ºç©ºåˆ™è®¤ä¸ºä¸å­˜åœ¨
    stripped = path.strip()
    if not stripped:
        return False

    # æœ€ç»ˆåˆ¤æ–­æ–‡ä»¶æˆ–ç›®å½•æ˜¯å¦å­˜åœ¨
    return os.path.exists(stripped)


def post_comments_once(commenter_list,
                       comment_list,
                       bvid,
                       max_success_comment_count,
                       comment_used_list,
                       path_exists,
                       max_workers=5,
                       jitter=(0.4, 1.0)):
    """
    æœ€ç»ˆä¿®è®¢ç‰ˆV3ï¼š
    1. ä½¿ç”¨ futures.wait() å®ç°å¯é çš„å…¨å±€è¶…æ—¶ã€‚
    2. ä¿®å¤äº† comment_used_list çš„åŒæ­¥BUGï¼Œåªè®°å½•çœŸæ­£æˆåŠŸçš„è¯„è®ºã€‚
    3. å°† jitter å»¶è¿Ÿæ”¾å› worker çº¿ç¨‹ä»¥å®ç°å¹¶å‘å»¶è¿Ÿã€‚
    4. ç¡®ä¿è¿”å›çš„ success_count æ˜¯åœ¨è¶…æ—¶å‰ç¡®å®šçš„å€¼ã€‚
    """
    # --- 1. å‡†å¤‡å·¥ä½œï¼šåˆ†é…è¯„è®ºä»»åŠ¡ (é€»è¾‘ä¿æŒä¸å˜) ---
    random.shuffle(commenter_list)
    selected = commenter_list[:max_success_comment_count]

    # é”å’Œå…±äº«çŠ¶æ€
    used_lock = threading.Lock()
    successful_texts_lock = threading.Lock()
    used_texts = set(comment_used_list)
    successful_texts = [] # åªå­˜å‚¨æœ¬æ¬¡è°ƒç”¨ä¸­æˆåŠŸå‘é€çš„è¯„è®ºæ–‡æœ¬

    assignments = []
    for c in selected:
        assigned = None
        # ä» comment_list ä¸­æ‰¾åˆ°ä¸€æ¡æœªè¢«ä½¿ç”¨çš„è¯„è®º
        for detail in comment_list:
            text = detail[0] if detail and len(detail) > 0 else None
            if not text or len(text) <= 1:
                continue

            with used_lock:
                if text in used_texts:
                    continue
                # é¢„å…ˆé”å®šï¼Œé˜²æ­¢è¢«å…¶ä»–ä»»åŠ¡åˆ†é…
                used_texts.add(text)
                assigned = detail
                break # æ‰¾åˆ°ä¸€æ¡å°±è·³å‡ºå†…å±‚å¾ªç¯

        if assigned:
            assignments.append((c, assigned))
        else:
            break # å¦‚æœæ‰¾ä¸åˆ°å¯ç”¨çš„è¯„è®ºäº†ï¼Œå°±åœæ­¢åˆ†é…

    if not assignments:
        print("æ²¡æœ‰å¯åˆ†é…çš„è¯„è®ºæˆ– commenterï¼Œé€€å‡ºã€‚")
        return 0

    # --- 2. Worker å‡½æ•°å®šä¹‰ (ä¿®è®¢ç‰ˆ) ---
    def worker(pair):
        # 4. jitteræ”¾å›workerï¼Œå®ç°å¹¶å‘å»¶è¿Ÿ
        time.sleep(random.uniform(*jitter))

        commenter, detail = pair
        text = detail[0]
        image_path = detail[2] if len(detail) > 2 else None

        try:
            # æ‰§è¡Œè¯„è®ºæ“ä½œ
            if image_path and path_exists(image_path):
                rpid = commenter.post_comment(bvid, text, 1, like_video=True, image_path=image_path, forward_to_dynamic=False)
            else:
                rpid = commenter.post_comment(bvid, text, 1, like_video=True, forward_to_dynamic=False)

            if rpid:
                # 3. åªæœ‰æˆåŠŸæ—¶ï¼Œæ‰å°†æ–‡æœ¬è®°å½•åˆ° successful_texts
                with successful_texts_lock:
                    successful_texts.append(text)
                name = commenter.all_params.get('name', 'unknown')
                print(f"[è¯„è®ºæˆåŠŸ] by {name} rpid:{rpid}: {text}")
                return True # è¿”å›æˆåŠŸçŠ¶æ€
            else:
                print(f"[è¯„è®ºå¤±è´¥] by {getattr(commenter, 'name', 'unknown')} (æ¥å£è¿”å›): {text}")
                return False # è¿”å›å¤±è´¥çŠ¶æ€

        except Exception as e:
            print(f"[è¯„è®ºå¼‚å¸¸] by {getattr(commenter, 'name', 'unknown')}: {text} -> {e}")
            return False # å¼‚å¸¸ä¹Ÿè§†ä¸ºå¤±è´¥
        finally:
            # æ— è®ºæˆåŠŸã€å¤±è´¥è¿˜æ˜¯å¼‚å¸¸ï¼Œéƒ½è¦ä»â€œé¢„é”å®šâ€é›†åˆä¸­é‡Šæ”¾
            # å› ä¸ºåªæœ‰ successful_texts é‡Œçš„æ‰ç®—çœŸæ­£â€œå·²ä½¿ç”¨â€
            with used_lock:
                if text in used_texts:
                    used_texts.remove(text)

    # ==========================================================
    # ==================== æ ¸å¿ƒæ‰§è¡ŒåŒºåŸŸ =======================
    # ==========================================================

    TOTAL_TIMEOUT = 300  # æ•´ä¸ªè¯„è®ºç¯èŠ‚æœ€å¤šæ‰§è¡Œ5åˆ†é’Ÿ
    executor = ThreadPoolExecutor(max_workers=min(max_workers, len(assignments)))

    # å°†ä»»åŠ¡å’ŒåŸå§‹ä¿¡æ¯å…³è”èµ·æ¥
    future_to_info = {executor.submit(worker, a): a[1][0] for a in assignments}

    # å±€éƒ¨å˜é‡ï¼Œç”¨äºç»Ÿè®¡åœ¨è¶…æ—¶å‰ç¡®è®¤çš„æˆåŠŸæ•°
    confirmed_success_count = 0

    try:
        done, not_done = wait(future_to_info.keys(), timeout=TOTAL_TIMEOUT, return_when=ALL_COMPLETED)

        # å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡
        for future in done:
            try:
                # è·å–workerçš„è¿”å›ç»“æœ (True/False)
                if future.result():
                    confirmed_success_count += 1
            except Exception:
                # workerå†…éƒ¨çš„å¼‚å¸¸å·²ç»è¢«æ•è·å¹¶è¿”å›Falseï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†ä»£ç å¥å£®æ€§
                pass

        # å¤„ç†æœªå®Œæˆ/è¶…æ—¶çš„ä»»åŠ¡
        if not_done:
            print(f"[è¯„è®ºæ€»è¶…æ—¶] {len(not_done)} ä¸ªä»»åŠ¡åœ¨ {TOTAL_TIMEOUT} ç§’åä»æœªå®Œæˆï¼Œå°†è¢«æ”¾å¼ƒã€‚")
            for future in not_done:
                text = future_to_info[future]
                print(f"  - è¶…æ—¶ä»»åŠ¡çš„è¯„è®º: '{text[:30]}...'")
                # ã€é‡è¦ã€‘è¶…æ—¶ä»»åŠ¡ä¹Ÿéœ€è¦ä»â€œé¢„é”å®šâ€é›†åˆä¸­é‡Šæ”¾ï¼Œworker çš„ finally æ— æ³•æ‰§è¡Œ
                with used_lock:
                    if text in used_texts:
                        used_texts.remove(text)

    finally:
        print(f"åœ¨è¶…æ—¶å‰ç¡®è®¤æˆåŠŸçš„è¯„è®ºæ•°: {confirmed_success_count}")
        # å°†æœ¬æ¬¡è°ƒç”¨ä¸­æ‰€æœ‰ç¡®è®¤æˆåŠŸçš„è¯„è®ºæ–‡æœ¬ï¼ŒåŒæ­¥å›åŸå§‹çš„listä¸­
        # è¿‡æ»¤æ‰å¯èƒ½é‡å¤çš„é¡¹
        new_successes = [text for text in successful_texts if text not in comment_used_list]
        comment_used_list.extend(new_successes)

        # ç«‹å³å…³é—­çº¿ç¨‹æ± ï¼Œä¸ç­‰å¾…åƒµå°¸çº¿ç¨‹
        executor.shutdown(wait=False)
        print("çº¿ç¨‹æ± å·²å‘å‡ºå…³é—­ä¿¡å·ï¼Œä¸»æµç¨‹ç»§ç»­ã€‚")

    return confirmed_success_count

def send_danmaku_thread_function(owner_commenter, owner_danmu_list, max_success_owner_danmu_count, bvid,
                                 owner_danmu_used_list):
    """
    è¿™ä¸ªå‡½æ•°åŒ…å«äº†å‘é€å¼¹å¹•çš„å®Œæ•´é€»è¾‘ï¼Œå®ƒå°†åœ¨ä¸€ä¸ªç‹¬ç«‹çš„çº¿ç¨‹ä¸­è¢«æ‰§è¡Œã€‚
    """
    success_owner_danmu_count = 0  # è®¡æ•°å™¨åœ¨çº¿ç¨‹å†…éƒ¨åˆå§‹åŒ–å’Œä½¿ç”¨
    if owner_commenter:
        for detail_owner_danmu in owner_danmu_list:
            if success_owner_danmu_count >= max_success_owner_danmu_count:
                print(f"çº¿ç¨‹ {threading.current_thread().name}: å·²è¾¾åˆ°æœ€å¤§æˆåŠŸUPä¸»å¼¹å¹•æ•°ï¼Œåœæ­¢å¤„ç†ã€‚")
                break

            danmaku_time_ms = detail_owner_danmu['å»ºè®®æ—¶é—´æˆ³'] * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            danmu_text_list = detail_owner_danmu['æ¨èå¼¹å¹•å†…å®¹']

            for danmu_text in danmu_text_list:
                if danmu_text in owner_danmu_used_list or len(danmu_text) == 0:
                    continue

                # å†æ¬¡æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œé¿å…åœ¨å†…å±‚å¾ªç¯ä¸­è¶…å‡º
                if success_owner_danmu_count >= max_success_owner_danmu_count:
                    break

                danmaku_sent = owner_commenter.send_danmaku(
                    bvid=bvid,
                    msg=danmu_text,
                    progress=danmaku_time_ms,
                    is_up=True
                )

                if danmaku_sent:
                    owner_danmu_used_list.append(danmu_text)
                    success_owner_danmu_count += 1
                    print(
                        f" [ä¸»äººå¼¹å¹•å‘é€æµç¨‹æˆåŠŸä¸ªæ•° {success_owner_danmu_count}] {danmu_text} BVID: {bvid} name {owner_commenter.all_params['name']}")
                    time.sleep(random.uniform(5, 10))
                else:
                    print(
                        f"{success_owner_danmu_count} ä¸»äººå¼¹å¹•å‘é€æµç¨‹å¤±è´¥ï¼{danmu_text} BVID: {bvid} name {owner_commenter.all_params['name']} danmaku_time_ms: {danmaku_time_ms}")
                    time.sleep(random.uniform(10, 15))

            # åœ¨å¤„ç†å®Œä¸€ä¸ªå¼¹å¹•åŒ…åç¨ä½œç­‰å¾…
            time.sleep(random.uniform(10, 15))
    print(f"çº¿ç¨‹ {threading.current_thread().name} å®Œæˆã€‚æˆåŠŸå‘é€ UP ä¸»å¼¹å¹•æ•°: {success_owner_danmu_count}")


def _send_danmu_worker(danmu_list, other_commenters, bvid, max_success_other_danmu_count, stop_event, result):
    try:
        random.shuffle(other_commenters)
        senders = deque(other_commenters)
        success_count = 0
        sent_texts = []

        for detail in danmu_list:
            if stop_event.is_set():
                print("send worker: æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºã€‚")
                break

            if success_count >= max_success_other_danmu_count:
                break

            danmaku_time_ms = int(detail.get('å»ºè®®æ—¶é—´æˆ³', 0) * 1000)
            danmu_text_list = detail.get('æ¨èå¼¹å¹•å†…å®¹', []) or []

            for text in danmu_text_list:
                if stop_event.is_set() or success_count >= max_success_other_danmu_count:
                    break
                if not text:
                    continue

                sender = senders.popleft()
                try:
                    danmaku_sent = sender.send_danmaku(
                        bvid=bvid,
                        msg=text,
                        progress=danmaku_time_ms,
                        is_up=False
                    )
                except Exception as e:
                    print("å‘é€å¼‚å¸¸:", e)
                    danmaku_sent = False

                # è½®è½¬å‘é€è€…
                senders.append(sender)

                if danmaku_sent:
                    success_count += 1
                    sent_texts.append(text)
                    # ä»…æ‰“å°ï¼Œä¸ä¿®æ”¹å¤–éƒ¨åˆ—è¡¨
                    print(f"[æˆåŠŸå¼¹å¹•ä¸ªæ•° {success_count}] {text} å‘é€è€…: {sender.all_params.get('name')}")
                else:
                    print(f"[å¤±è´¥] {text} å‘é€è€…: {sender.all_params.get('name')}ï¼Œç¨åç»§ç»­æˆ–è·³è¿‡ã€‚")
                    time.sleep(random.uniform(5, 10))

                # æ§åˆ¶é€Ÿç‡
                time.sleep(random.uniform(1, 2))

        result.success_count = success_count
        result.sent_texts = sent_texts
        print("send worker å®Œæˆã€‚æˆåŠŸå‘é€:", success_count)
    except Exception as e:
        print("worker æœªæ•è·å¼‚å¸¸:", e)
        result.exception = e

def start_send_danmu_background(danmu_list, other_commenters, bvid, max_success_other_danmu_count, daemon=True):
    """
    å¯åŠ¨åå°çº¿ç¨‹å‘é€å¼¹å¹•ï¼ˆæç®€ç‰ˆï¼‰ã€‚
    è¿”å› (thread, stop_event, result)ï¼š
      - thread: threading.Thread å¯¹è±¡
      - stop_event: threading.Eventï¼Œå¯ä»¥é€šè¿‡ stop_event.set() åœæ­¢çº¿ç¨‹
      - result: SimpleNamespaceï¼Œçº¿ç¨‹ç»“æŸååŒ…å« .success_count, .sent_texts, ä»¥åŠå¯é€‰çš„ .exception
    è¯´æ˜ï¼šè¯¥å®ç°ä¸ä¼šä¿®æ”¹å¤–éƒ¨çš„ danmu_used_list æˆ– hudong_infoï¼Œéœ€ä½ åœ¨ä¸»çº¿ç¨‹ä¸­è‡ªè¡Œå¤„ç†ã€‚
    """
    stop_event = threading.Event()
    result = SimpleNamespace(success_count=0, sent_texts=[])
    t = threading.Thread(
        target=_send_danmu_worker,
        args=(danmu_list, other_commenters, bvid, max_success_other_danmu_count, stop_event, result),
        daemon=daemon
    )
    t.start()
    return t, stop_event, result


def pick_commenters(commenter_map, usage_path, n=3):
    """
    ä» commenter_map ä¸­å°½é‡å‡åŒ€é€‰ n ä¸ªè´¦å·ï¼Œè¯»å–/æ›´æ–° usage_pathã€‚
    ç‰¹æ®Š uid ä½¿ç”¨ä¸€æ¬¡è®° 2 æ¬¡ï¼Œå…¶ä»–è®° 1 æ¬¡ã€‚
    è¿”å›é€‰ä¸­çš„ commenter å¯¹è±¡åˆ—è¡¨ã€‚
    """
    usage_map = {'196823511':6,'3546972143225467':4,'3546717871934392':5,'3632304865937878':2, '3546970887031023':3, '3546979686681114':3, '3546970725550911':3, '3632307990694238':3}

    usage = read_json(usage_path) or {}
    # ensure keys are strings
    usage = {str(k): int(v) for k,v in usage.items()}
    for uid in list(commenter_map.keys()):
        usage.setdefault(str(uid), 0)

    # éšæœºæ‰“ä¹±åæŒ‰ä½¿ç”¨æ¬¡æ•°å‡åºé€‰æ‹©ï¼Œæ‰“ç ´å¹¶åˆ—çš„ç¡®å®šæ€§
    uids = list(map(str, commenter_map.keys()))
    random.shuffle(uids)
    uids.sort(key=lambda x: usage.get(x, 0))

    selected = uids[:min(n, len(uids))]
    for uid in selected:
        usage[uid] = usage.get(uid, 0) + 8 - usage_map.get(uid, 2)

    save_json(usage_path, usage)
    selected_commenter = [commenter_map[uid] for uid in selected if uid in commenter_map]
    return selected_commenter


def process_single_video(bvid, hudong_info, uid, commenter_map, today=None):
    # --- æ–°å¢ï¼šä¸ºçº¿ç¨‹ç­‰å¾…å®šä¹‰ç»Ÿä¸€çš„è¶…æ—¶æ—¶é—´ (å•ä½ï¼šç§’) ---
    THREAD_JOIN_TIMEOUT = 900  # 15åˆ†é’Ÿ

    print(f"[{bvid}] --- process_single_video å¼€å§‹ ---")

    if not today:
        today = datetime.date.today().isoformat()
    if hudong_info.get('last_processed_date'):
        print(f"[{bvid}] è·³è¿‡ï¼šè¯¥è§†é¢‘å·²æœ‰å¤„ç†æ—¥æœŸã€‚")
        return hudong_info, True
    if hudong_info.get('last_processed_date') == today:
        hudong_info['last_processed_date_count'] = hudong_info.get('last_processed_date_count', 0)
        if hudong_info['last_processed_date_count'] >= 1:
            print(f"[{bvid}] è·³è¿‡ï¼šä»Šå¤©å·²å¤„ç†è¿‡ {hudong_info['last_processed_date_count']} æ¬¡ã€‚")
            return hudong_info, True

    print(f"[{bvid}] [æ­¥éª¤ 1/8] è°ƒç”¨ gen_proper_comment è·å–å·²æœ‰äº’åŠ¨ä¿¡æ¯...")
    result = gen_proper_comment(bvid, dont_need_comment=True)
    print(f"[{bvid}] [æ­¥éª¤ 1/8] gen_proper_comment è°ƒç”¨å®Œæˆã€‚")

    exist_comment = result.get('å·²æœ‰è¯„è®º', [])
    exist_comment_text = [comment[0] for comment in exist_comment]
    exist_danmu = result.get('å·²æœ‰å¼¹å¹•', [])
    exist_danmu_text = [danmu[0] for danmu in exist_danmu]
    max_success_comment_count = 2
    max_success_owner_danmu_count = 5
    max_success_other_danmu_count = 5

    print(f"è·å¾—åˆ°å·²æœ‰è¯„è®ºï¼š{len(exist_comment_text)} æ¡ï¼Œå·²æœ‰å¼¹å¹•ï¼š{len(exist_danmu_text)} æ¡ã€‚| BVID: {bvid}")
    owner_commenter = commenter_map.get(uid, None)
    other_commenters = [c for k, c in commenter_map.items() if k != uid]
    share_video = hudong_info.get("share_video", False)
    triple_like_video = hudong_info.get("triple_like_video", False)

    # åˆå§‹åŒ– watch_thread å˜é‡ï¼Œé˜²æ­¢åç»­å¼•ç”¨æŠ¥é”™
    watch_thread = None

    print(f"[{bvid}] [æ­¥éª¤ 2/8] æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†äº«å’Œä¸‰è¿...")
    if not share_video or not triple_like_video:
        print(f"[{bvid}] [æ­¥éª¤ 2a/8] éœ€è¦æ‰§è¡Œåˆ†äº«/ä¸‰è¿ã€‚æ­£åœ¨å¯åŠ¨ watch_video åå°çº¿ç¨‹...")

        # --- ä¿®æ”¹ç‚¹ï¼šå°† watch_video æ”¾å…¥åå°çº¿ç¨‹å¯åŠ¨ ---
        try:
            watch_thread = threading.Thread(
                target=watch_video,
                args=([bvid],)
            )
            watch_thread.start()
            print(f"[{bvid}] [æ­¥éª¤ 2a/8] watch_video åå°çº¿ç¨‹å·²å¯åŠ¨ï¼Œä¸»ç¨‹åºç»§ç»­æ‰§è¡Œåˆ†äº«æ“ä½œã€‚")
        except Exception as e:
            print(f"[{bvid}] å¯åŠ¨ watch_video çº¿ç¨‹å¤±è´¥: {e}")
        # -------------------------------------------

        for commenter in commenter_map.values():
            name = commenter.all_params.get('name', 'unknown')
            print(f"[{bvid}] [æ­¥éª¤ 2b/8] ç”¨æˆ· '{name}' æ­£åœ¨æ‰§è¡Œ share_video...")
            share_success = commenter.share_video(bvid=bvid)
            if share_success:
                share_video = True
            else:
                print(f"[{bvid}] ç”¨æˆ· '{name}' åˆ†äº«æ“ä½œæµç¨‹å¤±è´¥ã€‚")
            print(f"[{bvid}] [æ­¥éª¤ 2b/8] ç”¨æˆ· '{name}' share_video è°ƒç”¨å®Œæˆã€‚")

            print(f"[{bvid}] [æ­¥éª¤ 2c/8] ç”¨æˆ· '{name}' æ­£åœ¨æ‰§è¡Œ triple_like_video...")
            triple_like_success = commenter.triple_like_video(bvid=bvid)
            if triple_like_success:
                triple_like_video = True
            else:
                print(f"[{bvid}] ç”¨æˆ· '{name}' ä¸€é”®ä¸‰è¿æ“ä½œæµç¨‹å¤±è´¥ã€‚")
            print(f"[{bvid}] [æ­¥éª¤ 2c/8] ç”¨æˆ· '{name}' triple_like_video è°ƒç”¨å®Œæˆã€‚")

        max_success_comment_count = 20
        max_success_owner_danmu_count = 20
        max_success_other_danmu_count = 30
    print(f"[{bvid}] [æ­¥éª¤ 2/8] åˆ†äº«å’Œä¸‰è¿æ“ä½œæ£€æŸ¥å®Œæˆï¼ˆè§‚çœ‹ä»»åŠ¡å¯èƒ½ä»åœ¨åå°è¿›è¡Œï¼‰ã€‚")

    hudong_info['share_video'] = share_video
    hudong_info['triple_like_video'] = triple_like_video
    owner_danmu_list = hudong_info.get('owner_danmu', [])
    owner_danmu_used_list = hudong_info.get('owner_danmu_used', [])
    owner_danmu_used_list.extend(exist_danmu_text)
    danmaku_thread = None

    print(f"[{bvid}] [æ­¥éª¤ 3/8] å‡†å¤‡å¯åŠ¨ä¸»äººå¼¹å¹•çº¿ç¨‹...")
    if owner_commenter:
        danmaku_thread = threading.Thread(
            target=send_danmaku_thread_function,
            args=(
                owner_commenter,
                owner_danmu_list,
                max_success_owner_danmu_count,
                bvid,
                owner_danmu_used_list
            )
        )
        danmaku_thread.start()
        print(f"[{bvid}] [æ­¥éª¤ 3/8] ä¸»äººå¼¹å¹•çº¿ç¨‹å·²å¯åŠ¨ã€‚")
    else:
        print(f"[{bvid}] [æ­¥éª¤ 3/8] æ— ä¸»äººè¯„è®ºè€…ï¼Œè·³è¿‡å¯åŠ¨ä¸»äººå¼¹å¹•çº¿ç¨‹ã€‚")

    danmu_list = hudong_info.get('danmu_list', [])
    danmu_used_list = hudong_info.get('danmu_used', [])
    danmu_used_list.extend(exist_danmu_text)

    print(f"[{bvid}] [æ­¥éª¤ 4/8] å‡†å¤‡å¯åŠ¨å…¶ä»–ç”¨æˆ·å¼¹å¹•çº¿ç¨‹...")
    t, stop_event, result = start_send_danmu_background(danmu_list, other_commenters, bvid,
                                                        max_success_other_danmu_count)
    print(f"[{bvid}] [æ­¥éª¤ 4/8] å…¶ä»–ç”¨æˆ·å¼¹å¹•çº¿ç¨‹å·²å¯åŠ¨ã€‚")

    max_success_comment_count = 5
    if uid in ['3632307990694238', '3632313749473288', '3632309148322699']:
        max_success_comment_count = 10
    comment_list = hudong_info.get('comment_list', [])
    comment_used_list = hudong_info.get('comment_used', [])
    comment_used_list.extend(exist_comment_text)

    print(f"[{bvid}] [æ­¥éª¤ 5/8] è°ƒç”¨ pick_commenters é€‰æ‹©è¯„è®ºè€…...")
    comment_commenters = pick_commenters(commenter_map, '../../LLM/TikTokDownloader/back_up/commenter_usage.json',
                                         n=max_success_comment_count)
    print(f"[{bvid}] [æ­¥éª¤ 5/8] pick_commenters è°ƒç”¨å®Œæˆï¼Œé€‰æ‹©äº† {len(comment_commenters)} ä¸ªè¯„è®ºè€…ã€‚")

    print(f"[{bvid}] [æ­¥éª¤ 6/8] å‡†å¤‡è°ƒç”¨ post_comments_once å‘é€è¯„è®º...")
    post_comments_once(
        commenter_list=comment_commenters,
        comment_list=comment_list,
        bvid=bvid,
        max_success_comment_count=max_success_comment_count,
        comment_used_list=comment_used_list,
        path_exists=path_exists,
        max_workers=5,
        jitter=(0.4, 1.0)
    )
    print(f"[{bvid}] [æ­¥éª¤ 6/8] post_comments_once è°ƒç”¨å®Œæˆã€‚")

    hudong_info['comment_used'] = comment_used_list
    if hudong_info.get('last_processed_date') == today:
        last_count = int(hudong_info.get('last_processed_date_count', 0) or 0)
        hudong_info['last_processed_date_count'] = last_count + 1
    else:
        hudong_info['last_processed_date_count'] = 1
    hudong_info['last_processed_date'] = today

    print(f"[{bvid}] [æ­¥éª¤ 7/8] å‡†å¤‡ç­‰å¾…ä¸»äººå¼¹å¹•çº¿ç¨‹...")
    if danmaku_thread and danmaku_thread.is_alive():
        danmaku_thread.join(timeout=THREAD_JOIN_TIMEOUT)
        if danmaku_thread.is_alive():
            print(f"[{bvid}] è­¦å‘Šï¼šä¸»äººå¼¹å¹•çº¿ç¨‹åœ¨ {THREAD_JOIN_TIMEOUT} ç§’åä»æœªç»“æŸã€‚")
        else:
            print(f"[{bvid}] ä¸»äººå¼¹å¹•çº¿ç¨‹å·²æˆåŠŸæ‰§è¡Œå®Œæ¯•ã€‚")
    else:
        print(f"[{bvid}] ä¸»äººå¼¹å¹•ä»»åŠ¡æœªå¯åŠ¨æˆ–å·²æ‰§è¡Œå®Œæ¯•ã€‚")
    print(f"[{bvid}] [æ­¥éª¤ 7/8] ä¸»äººå¼¹å¹•çº¿ç¨‹ç­‰å¾…å®Œæˆã€‚")

    hudong_info['owner_danmu_used'] = owner_danmu_used_list

    print(f"[{bvid}] [æ­¥éª¤ 8/8] å‡†å¤‡ç­‰å¾…å…¶ä»–ç”¨æˆ·å¼¹å¹•çº¿ç¨‹...")
    t.join(timeout=THREAD_JOIN_TIMEOUT)
    if t.is_alive():
        print(f"[{bvid}] è­¦å‘Šï¼šå…¶ä»–ç”¨æˆ·å¼¹å¹•çº¿ç¨‹åœ¨ {THREAD_JOIN_TIMEOUT} ç§’åä»æœªç»“æŸã€‚")
        stop_event.set()
    else:
        print(f"[{bvid}] å…¶ä»–ç”¨æˆ·å¼¹å¹•çº¿ç¨‹å·²æˆåŠŸæ‰§è¡Œå®Œæ¯•ã€‚")
    print(f"[{bvid}] [æ­¥éª¤ 8/8] å…¶ä»–ç”¨æˆ·å¼¹å¹•çº¿ç¨‹ç­‰å¾…å®Œæˆã€‚")

    hudong_info['danmu_used'] = result.sent_texts

    # --- æ–°å¢ï¼šæœ€åç­‰å¾… watch_video çº¿ç¨‹ç»“æŸ ---
    if watch_thread:
        print(f"[{bvid}] å‡†å¤‡ç­‰å¾… watch_video åå°çº¿ç¨‹...")
        if watch_thread.is_alive():
            watch_thread.join(timeout=THREAD_JOIN_TIMEOUT)
            if watch_thread.is_alive():
                print(f"[{bvid}] è­¦å‘Šï¼šwatch_video çº¿ç¨‹åœ¨ {THREAD_JOIN_TIMEOUT} ç§’åä»æœªç»“æŸã€‚")
            else:
                print(f"[{bvid}] watch_video çº¿ç¨‹å·²æˆåŠŸæ‰§è¡Œå®Œæ¯•ã€‚")
        else:
            print(f"[{bvid}] watch_video çº¿ç¨‹æ­¤å‰å·²è‡ªåŠ¨å®Œæˆã€‚")
    # ---------------------------------------

    print(f"[{bvid}] --- process_single_video ç»“æŸ ---")
    return hudong_info, False


def fix_metadata_cache_with_uploads(all_found_videos, metadata_cache_with_uploads):
    for video in all_found_videos:
        title = video.get('title', '')

        bvid = video.get('bvid', '')
        video_info = find_video_by_title(title, metadata_cache_with_uploads)
        if video_info:
            if video_info['upload_info']['upload_result']['bvid'] != bvid:
                print(f"ä¿®æ­£è§†é¢‘æ ‡é¢˜ {title} çš„ BVID: {video_info['upload_info']['upload_result']['bvid']} -> {bvid}")
                video_info['upload_info']['upload_result']['bvid'] = bvid
                save_json('../../LLM/TikTokDownloader/metadata_cache_with_uploads.json', metadata_cache_with_uploads)


stop_event = threading.Event()

NEED_UPDATE_SIGN = True
signatures = [
    "è°¢è°¢ä½ è¿™ä¹ˆå¥½çœ‹è¿˜æ¥çœ‹çœ‹æˆ‘ï¼Œæ„¿ä½ æ¯å¤©éƒ½è¢«æ¸©æŸ”å¯¹å¾…ã€‚",
    "èƒ½é‡è§ä½ çœŸå¥½ï¼Œç¥ä½ ç¬‘å£å¸¸å¼€ã€‚",
    "ä½ çœ‹æˆ‘ä¸€çœ¼ï¼Œæˆ‘å°±æŠŠå¥½è¿ç»™ä½ ç•™ç€ã€‚",
    "çœ‹åˆ°ä½ çœŸæš–ï¼Œæ„¿ä½ çš„æ¯ä¸€å¤©éƒ½æ™´æœ—ã€‚",
    "è°¢è°¢ä½ åœç•™ï¼Œæ„¿å¿«ä¹æ‰¾ä¸Šé—¨ã€‚",
    "å› ä¸ºæœ‰ä½ ï¼Œæˆ‘çš„ä¸–ç•Œæ›´äº®ã€‚",
    "ä½ è¿™ä¹ˆæ£’ï¼Œåˆ«å¿˜äº†å¯¹è‡ªå·±å¥½ä¸€ç‚¹ã€‚",
    "æ„Ÿè°¢ä½ çš„å…³æ³¨ï¼Œæ„¿ä½ å¿ƒæƒ³äº‹æˆã€‚",
    "è°¢è°¢ä½ æ¥çœ‹æˆ‘ï¼Œæ„¿ä½ å¤œå¤œå¥½æ¢¦ã€‚",
    "ä½ å¥½å¯çˆ±ï¼Œè°¢è°¢ä½ æ¥ï¼Œæ„¿ä½ äº‹äº‹é¡ºå¿ƒã€‚",
    "ä½ çš„å‡ºç°ï¼Œè®©æˆ‘çš„å¿ƒæƒ…å˜å¥½äº†ã€‚",
    "ä½ æ¥è¿‡ï¼Œæˆ‘å°±è¶³å¤Ÿå¹¸ç¦äº†ã€‚",
    "çœ‹è§ä½ å°±æƒ³ç¬‘ï¼Œæ„¿ä½ æ°¸è¿œè¢«å–œæ¬¢ã€‚",
    "è°¢è°¢ä½ æ¸©æŸ”ä»¥å¾…ï¼Œæ„¿ä½ è¢«ç”Ÿæ´»æ¸©æŸ”ç›¸å¾…ã€‚",
    "æœ‰ä½ åœ¨ï¼Œå¹³å‡¡ä¹Ÿå˜æœ‰è¶£ã€‚",
    "é‡è§ä½ æ˜¯æœ€å¥½çš„å·§åˆï¼Œç¥ä½ å®‰å¥½ã€‚",
    "ä½ çš„å¾®ç¬‘å¾ˆæš–ï¼Œè°¢è°¢ä½ åœç•™ã€‚",
    "è°¢è°¢ä½ æŠŠæ—¶é—´å€Ÿç»™æˆ‘ï¼Œæ„¿ä½ è¢«ä¸–ç•Œæ¸©æŸ”ä»¥å¾…ã€‚",
    "ä½ åœ¨çš„åœ°æ–¹å°±æœ‰å…‰ï¼Œæ„¿ä½ å‰è·¯æ— å¿§ã€‚",
    "æ„Ÿè°¢ä»Šå¤©çš„ç›¸é‡ï¼Œæ„¿ä½ ä¸€ç›´å¥½è¿è¿è¿ã€‚",
    "è°¢è°¢ä½ æ¥çœ‹çœ‹ï¼Œæ„¿æ‰€æœ‰å°ç¡®å¹¸éƒ½å‘ä½ é è¿‘ã€‚",
    "è°¢è°¢ä½ ä¸ºæˆ‘ç‚¹äº®ä¸€çœ¼ï¼Œæ„¿ä½ æ¯å¤©è¢«å¹¸è¿å® çˆ±ã€‚",
    "ä½ çš„å¥½çœ‹å€¼å¾—è¢«ä¸–ç•Œèµç¾ï¼Œç¥ä½ è¢«çˆ±åŒ…å›´ã€‚",
    "å› ä¸ºä½ ï¼Œå¹³æ·¡ä¹Ÿå˜æˆä»ªå¼æ„Ÿã€‚",
    "ä½ çš„å‡ºç°ï¼Œè®©æˆ‘ç›¸ä¿¡ç¾å¥½è¿˜åœ¨ã€‚",
    "è°¢è°¢ä½ è¿™ä¹ˆæ¸©æŸ”åœ°çœ‹æˆ‘ï¼Œæ„¿ä½ æ°¸è¿œè¢«æ¸©æŸ”ç›¸å¾…ã€‚",
    "ä½ æŠŠå¥½å¿ƒæƒ…å¸¦æ¥ï¼Œæˆ‘æŠŠç¥ç¦é€ç»™ä½ ã€‚",
    "æœ‰ä½ ç‚¹èµçœŸå¼€å¿ƒï¼Œæ„¿ä½ æ­¤åˆ»å¿«ä¹ã€‚",
    "è°¢è°¢ä½ è·¯è¿‡æˆ‘çš„ä¸–ç•Œï¼Œæ„¿ä½ æ°¸è¿œå¿ƒå¹³æ°”å’Œã€‚"
]


def filter_recent_data(data_dict, days=10):
    """
    æ ¹æ® last_processed_date ä¿ç•™æœ€è¿‘ N å¤©çš„æ•°æ®ã€‚
    å…¼å®¹ import datetime çš„å¯¼å…¥æ–¹å¼ã€‚
    """
    # 1. è®¡ç®—æ—¶é—´é˜ˆå€¼
    now = datetime.datetime.now()
    cutoff_date = now - datetime.timedelta(days=days)

    # 2. éå†å¹¶è¿‡æ»¤
    filtered_data = {}
    for key, val in data_dict.items():
        # å°è¯•è·å–æ—¥æœŸå­—ç¬¦ä¸²
        date_str = val.get('hudong', {}).get('last_processed_date')
        if date_str:
            try:
                # å°†å­—ç¬¦ä¸²è§£æä¸ºæ—¶é—´å¯¹è±¡
                dt = datetime.datetime.strptime(date_str, "%Y-%m-%d")
                # ä¿ç•™ å¤§äºç­‰äº æˆªæ­¢æ—¶é—´çš„æ•°æ®
                if dt >= cutoff_date:
                    filtered_data[key] = val
            except ValueError:
                # å¦‚æœæ—¥æœŸæ ¼å¼ä¸å¯¹ï¼Œé»˜è®¤è·³è¿‡
                continue

    # 3. æ‰“å°é«˜ä¿¡æ¯å¯†åº¦æ—¥å¿—
    total_cnt = len(data_dict)
    kept_cnt = len(filtered_data)
    dropped_cnt = total_cnt - kept_cnt
    drop_rate = (dropped_cnt / total_cnt * 100) if total_cnt > 0 else 0

    # æ—¥å¿—åŒ…å«ï¼šå½“å‰æ—¶é—´ã€æˆªæ­¢æ—¥æœŸã€è¾“å…¥->è¾“å‡ºå˜åŒ–ã€ç§»é™¤æ•°é‡åŠå æ¯”
    print(f"[FilterLog] {now.strftime('%Y-%m-%d %H:%M:%S')} | "
          f"Cutoff: {cutoff_date.strftime('%Y-%m-%d')} (Past {days}d) | "
          f"Items: {total_cnt} -> {kept_cnt} (Dropped {dropped_cnt}, {drop_rate:.1f}%)")

    return filtered_data

def fun(manager):
    global NEED_UPDATE_SIGN
    try:
        now = datetime.now()
        pre_midnight = now.replace(hour=0, minute=0, second=0, microsecond=0)
        # å¾€å‰å‡å»ä¸¤å¤©
        pre_midnight = pre_midnight - timedelta(days=2)

        # æŸ¥è¯¢ä»Šæ—¥å·²æŠ•ç¨¿çš„ä»»åŠ¡
        recent_uploaded_tasks = manager.find_tasks_after_time_with_status(pre_midnight, [TaskStatus.UPLOADED])

        processed_count = 0
        print("å¼€å§‹æ‰§è¡Œ fun å‡½æ•°...å½“å‰æ—¶é—´:", datetime.datetime.now().isoformat())
        stop_event.clear()  # æ¸…é™¤åœæ­¢äº‹ä»¶
        today = datetime.date.today().isoformat()
        # åŠ è½½all_emote.json
        all_emote_list = load_processed_dict(ALL_BILIBILI_EMOTE_PATH)
        config_map = init_config()
        commenter = BilibiliCommenter(total_cookie=total_cookie, csrf_token=csrf_token)
        commenter_map = {}
        for key, detail_config in config_map.items():
            name = detail_config.get('name', key)
            # if name in ['mama']:
            #     continue
            cookie = detail_config.get('total_cookie', '')
            all_params = detail_config.get('all_params', {})
            commenter_map[key] = BilibiliCommenter(
                total_cookie=cookie,
                csrf_token=detail_config.get('BILI_JCT', ''),
                all_params=all_params,
            )
            print(f"å·²åˆ›å»ºè¯„è®ºè€… {name} (UID: {key})")
        print(f"å…±åˆ›å»º {len(commenter_map)} ä¸ªè¯„è®ºè€…å®ä¾‹ã€‚")

        bvid_file_data = load_processed_dict(bvid_file_path)
        all_bvid_file_data = load_processed_dict(all_bvid_file_path)

        bvid_uid_map = {}
        all_found_videos = []
        for uid in config_map.keys():
            name = config_map[uid].get('name', uid)
            # if uid in ['3546965562362625']:
            #     continue
            # if name in ['hao', 'shuijun1', 'shuijun2', 'shuijun3', 'xiaodan', 'xiaoxiaosu', 'ruruxiao']:
            #     continue

            if NEED_UPDATE_SIGN:
                detail_config = config_map[uid]
                signature = random.choice(signatures)
                cookie = detail_config.get('total_cookie', '')
                result = update_bili_user_sign(cookie,signature)
                print(f"æ›´æ–°ç”¨æˆ·ç­¾åç»“æœ: {result}")

            logging.info(f"  > æ­£åœ¨è·å–UPä¸»(UID: {uid} {name})çš„æœ€æ–°åŠ¨æ€...")
            temp_found_videos = commenter.get_user_videos(mid=uid, desired_count=25)
            bvid_uid_map.update({video.get('bvid'): uid for video in temp_found_videos if 'bvid' in video})
            all_found_videos.extend(temp_found_videos)
            bvid_file_data[name] = temp_found_videos
            for video in temp_found_videos:
                all_bvid_file_data[video.get('bvid')] = video

            save_json(all_bvid_file_path, all_bvid_file_data)
            save_json(bvid_file_path, bvid_file_data)
        NEED_UPDATE_SIGN = False
        all_found_videos.sort(key=lambda x: x.get('created', 0), reverse=True)
        # åªä¿ç•™æœ€è¿‘1å°æ—¶çš„è§†é¢‘
        one_hour_ago = time.time() - 3600 * 3
        all_found_videos = [video for video in all_found_videos if video.get('created', 0) >= one_hour_ago]

        all_found_videos = all_found_videos
        print(f"å…±æ‰¾åˆ° {len(all_found_videos)} ä¸ªè§†é¢‘ã€‚")
        count = 0
        for video in all_found_videos:
            print(f"æ­£åœ¨å¤„ç†è§†é¢‘ BVID: {video.get('bvid', 'æœªçŸ¥')}...")
            count += 1
            start_time = time.time()
            bvid = video.get('bvid')
            uid = bvid_uid_map.get(bvid, 'æœªçŸ¥UID')
            hudong_info = gen_hudong_info(bvid, interaction_data, metadata_cache_with_uploads, all_emote_list)
            if hudong_info == {}:
                print(f"æ— äº’åŠ¨ä¿¡æ¯è·³è¿‡{bvid}")
                continue

            hudong_info, is_skip = process_single_video(bvid, hudong_info, uid, commenter_map, today)
            if not is_skip:
                processed_count += 1
            interaction_data[bvid] = {'hudong': hudong_info}
            save_json(interaction_data_file, interaction_data)
            print(
                f"è§†é¢‘ {bvid} çš„äº’åŠ¨ä¿¡æ¯å·²ç”Ÿæˆå¹¶ä¿å­˜ã€‚è€—æ—¶: {time.time() - start_time:.2f} ç§’ è¿›åº¦: {count}/{len(all_found_videos)} {datetime.datetime.now().isoformat()}")
            if stop_event.is_set():
                print("æ£€æµ‹åˆ°åœæ­¢è¯·æ±‚ï¼Œé€€å‡ºå½“å‰ä»»åŠ¡...")
                return  # åœæ­¢å½“å‰æ‰§è¡Œï¼Œé€€å‡º
        print(
            f"æ‰€æœ‰è§†é¢‘å¤„ç†æ‰€æœ‰å®Œæˆæ‰€æœ‰ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®..å½“å‰æ—¶é—´: {datetime.datetime.now().isoformat()} å…±å¤„ç† {processed_count} ä¸ªè§†é¢‘ã€‚å…±æ‰¾åˆ° {len(all_found_videos)} ä¸ªè§†é¢‘")
    except Exception as e:
        traceback.print_exc()
    finally:
        stop_event.set()  # æ ‡è®°ä»»åŠ¡ç»“æŸ


def run_periodically(manager):
    while True:
        loop_start = time.time()  # è®°å½•æœ¬è½® fun å¼€å§‹æ—¶é—´

        stop_event.set()
        fun_thread = threading.Thread(target=fun, args=(manager,))
        fun_thread.start()
        fun_thread.join()

        elapsed = time.time() - loop_start
        remaining = max(0, 30 * 60 - elapsed)  # å‰©ä½™ç­‰å¾…æ—¶é—´
        print(f"fun æ‰§è¡Œè€—æ—¶ {elapsed:.2f} ç§’ï¼Œç­‰å¾… {remaining:.2f} ç§’åå†æ‰§è¡Œä¸‹ä¸€è½®...")
        if remaining > 0:
            time.sleep(remaining)


if __name__ == '__main__':
    mongo_base_instance = gen_db_object()
    manager = MongoManager(mongo_base_instance)
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡çº¿ç¨‹
    threading.Thread(target=run_periodically, args=(manager,), daemon=True).start()

    # ä¸»çº¿ç¨‹å¯ç”¨äºå…¶ä»–ä»»åŠ¡ï¼Œæˆ–è€…ç»§ç»­ä¿æŒç¨‹åºè¿è¡Œ
    while True:
        time.sleep(10)
